{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Pinecone?\n",
    "Pinecone is a managed vector database that lets you efficiently store and search through high-dimensional vector embeddings. It's particularly useful for applications like semantic search, recommendation systems, and conversational AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAIEmbeddings(client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000251F7E6C9E0>, model='models/embedding-001', task_type=None, google_api_key=None, credentials=None, client_options=None, transport=None, request_options=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = embedding_model.embed_query(\"DataSciLearn\")\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"RAG.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'RAG.pdf', 'page': 0}, page_content='Adaptive-RAG: Learning to Adapt Retrieval-Augmented\\nLarge Language Models through Question Complexity\\nSoyeong Jeong1Jinheon Baek2Sukmin Cho1Sung Ju Hwang1,2Jong C. Park1*\\nSchool of Computing1Graduate School of AI2\\nKorea Advanced Institute of Science and Technology1,2\\n{starsuzi,jinheon.baek,nelllpic,sjhwang82,jongpark}@kaist.ac.kr\\nAbstract\\nRetrieval-Augmented Large Language Models\\n(LLMs), which incorporate the non-parametric\\nknowledge from external knowledge bases into\\nLLMs, have emerged as a promising approach\\nto enhancing response accuracy in several tasks,\\nsuch as Question-Answering (QA). However,\\neven though there are various approaches deal-\\ning with queries of different complexities, they\\neither handle simple queries with unnecessary\\ncomputational overhead or fail to adequately\\naddress complex multi-step queries; yet, not\\nall user requests fall into only one of the sim-\\nple or complex categories. In this work, we\\npropose a novel adaptive QA framework that\\ncan dynamically select the most suitable strat-\\negy for (retrieval-augmented) LLMs from the\\nsimplest to the most sophisticated ones based\\non the query complexity. Also, this selec-\\ntion process is operationalized with a classi-\\nfier, which is a smaller LM trained to predict\\nthe complexity level of incoming queries with\\nautomatically collected labels, obtained from\\nactual predicted outcomes of models and in-\\nherent inductive biases in datasets. This ap-\\nproach offers a balanced strategy, seamlessly\\nadapting between the iterative and single-step\\nretrieval-augmented LLMs, as well as the no-\\nretrieval methods, in response to a range of\\nquery complexities. We validate our model\\non a set of open-domain QA datasets, cov-\\nering multiple query complexities, and show\\nthat ours enhances the overall efficiency and\\naccuracy of QA systems, compared to rele-\\nvant baselines including the adaptive retrieval\\napproaches. Code is available at: https://\\ngithub.com/starsuzi/Adaptive-RAG .\\n1 Introduction\\nRecent Large Language Models (LLMs) (Brown\\net al., 2020; OpenAI, 2023; Touvron et al., 2023;\\nAnil et al., 2023) have shown overwhelming per-\\nformances across diverse tasks, including question-\\n*Corresponding author\\n0.5 1.0 1.5 2.0 2.5 3.0 3.5\\nTime per Query4748495051Performance (F1)  No Retrieval\\n  Single-step Approach  Adaptive Retrieval  Multi-step Approach   Adaptive-RAG (Ours)Performance vs Time with GPT-3.5Figure 1: QA performance (F1) and efficiency (Time/Query)\\nfor different retrieval-augmented generation approaches. We\\nuse the GPT-3.5-Turbo-Instruct as the base LLM.\\nanswering (QA) (Yang et al., 2018; Kwiatkowski\\net al., 2019). However, they still generate factu-\\nally incorrect answers since their knowledge solely\\nrelies on their parametric memory (Kasai et al.,\\n2022; Mallen et al., 2023). Meanwhile, memoriz-\\ning all the (ever-changing) world knowledge may\\nnot be possible. To address this problem, retrieval-\\naugmented LLMs (Borgeaud et al., 2022; Izacard\\net al., 2023; Shi et al., 2023), which incorporate\\nnon-parametric knowledge into LLMs with addi-\\ntional retrieval modules, have gained much increas-\\ning attention. Specifically, these models access\\na knowledge base, which serves as an extensive\\nrepository of information across various subjects\\nand disciplines, to retrieve information relevant to\\nthe given input, and then incorporate the retrieved\\ninformation into LLMs, which enables them to stay\\naccurate and current with the world knowledge.\\nA particularly salient application of retrieval-\\naugmented LLMs is to handling QA tasks, whose\\ngoal is to provide correct answers in response to\\nuser queries, especially those of high complexity.\\nEarly work on retrieval-augmented LLMs focuses\\nprimarily on single-hop queries (Lazaridou et al.,\\n2022; Ram et al., 2023), whose answers are typ-\\nically found within a single document; therefore,\\nthis approach involves retrieving a relevant doc-\\nument based on the query and subsequently inte-\\ngrating this information into QA models to formu-\\nlate a response. However, unlike this single-hop\\nQA, some queries require connecting and aggregat-\\ning multiple documents, which are, furthermore,arXiv:2403.14403v2  [cs.CL]  28 Mar 2024'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='RetrievalComplex Query: \\nWhat currency is in \\nBilly Giles’ birthplace?Simple Query: \\nWhen is the birthday \\nof Michael F. Phelps?Documents\\nAnswer\\nDocuments\\nAnswer\\n(A) Single -Step Approach\\nInaccurate\\nRetrieval\\nRetrievalSimple Query: \\nWhen is the birthday \\nof Michael F. Phelps?Documents\\n(Intermediate) \\nAnswers\\n(B) Multi -Step Approach\\nInefficient\\nk times\\nComplex Query: \\nWhat currency is in \\nBilly Giles’ birthplace?Documents\\n(Intermediate) \\nAnswers\\nk timesStraightforward Query: \\nParis is the capital of what?(C) Our Adaptive Approach\\nAnswer\\nSimple Query: \\nWhen is the birthday \\nof Michael F. Phelps?Documents\\nAnswer\\nComplex Query: \\nWhat currency is in \\nBilly Giles’ birthplace?Documents\\n(Intermediate) \\nAnswers\\nk times Classifier\\nFigure 2: A conceptual comparison of different retrieval-augmented LLM approaches to question answering. (A) In response to\\na query, this single-step approach retrieves relevant documents and then generates an answer. However, it may not be sufficient\\nfor complex queries that require multi-step reasoning. (B) This multi-step approach iteratively retrieves documents and generates\\nintermediate answers, which is powerful yet largely inefficient for the simple query since it requires multiple accesses to both\\nLLMs and retrievers. (C) Our adaptive approach can select the most suitable strategy for retrieval-augmented LLMs, ranging\\nfrom iterative, to single, to even no retrieval approaches, based on the complexity of given queries determined by our classifier.\\noften not answerable through a single-step pro-\\ncess of retrieval-and-response. An example query\\nis ‘When did the people who captured Malakoff\\ncome to the region where Philipsburg is located?’,\\nwhich requires four reasoning steps to solve. There-\\nfore, to effectively handle such complex queries,\\nrecent studies have concentrated largely on multi-\\nstep and multi-reasoning QA, which requires itera-\\ntive accesses to both LLMs and retrievers multiple\\ntimes (Press et al., 2023; Trivedi et al., 2023), at\\nthe cost of heavy computational overheads.\\nYet, we should rethink: In a real-world scenario,\\nare all the requests from users complex? Instead,\\nusers might often ask simple and straightforward\\nquestions, while only occasionally asking complex\\nones. Specifically, a query such as ‘Paris is the\\ncapital of what?’ is likely to be asked more fre-\\nquently, compared to the aforementioned multi-\\nstep query, and this simpler query might also be\\neasily answered by the LLMs themselves, without\\naccessing external knowledge. In other words, a\\nmulti-step QA approach could give rise to unnec-\\nessary computational overhead for simple queries,\\neven though it would be vital for complex queries\\n(see Figure 2 (A)). On the other hand, handling\\ncomplex queries with single-step-retrieval or even\\nnon-retrieval strategies would be largely insuffi-\\ncient (Figure 2 (B)). This suggests the need for an\\nadaptive QA system, which can dynamically adjust\\nthe operational strategies of retrieval-augmented\\nLLMs based on the query complexity. While some\\nrecent approaches are capable of doing this based\\non the frequency of entities in queries (Mallen et al.,\\n2023) or on the generated outputs from models\\nfor multi-step QA (Trivedi et al., 2023), they are\\nstill suboptimal: the former methods are overly\\nsimplistic, failing to consider multi-hop queries;\\nmeanwhile, the latter are excessively complex, ter-\\nminating answer solving steps after several rounds\\nof module access.In this work, considering diverse complexity lev-\\nels of real-world queries, we argue that previous\\none-size-fits-all approaches might be inadequate to\\ncover all of them. Instead, we propose to select the\\nmost suitable strategy from a range of (retrieval-\\naugmented) LLMs, each of which is tailored to the\\nspecific complexity of the input query. Notably,\\na critical step in this process is pre-defining the\\nquery complexity, which is instrumental in deter-\\nmining the most fitting model to it. In this work,\\nwe operationalize this process with a novel classi-\\nfier, which is a smaller model trained to predict the\\ncomplexity level of incoming queries (see Figure 2\\n(c)). Moreover, we automatically collect its training\\ndatasets without human labeling, by leveraging the\\npredicted outcomes (i.e., which models accurately\\nrespond to which queries) as well as by capitalizing\\non the inherent biases in existing datasets (i.e., sam-\\nples in the datasets are designed either for single-\\nstep or for multi-step QA scenarios). This proposed\\nmethod can offer a robust middle ground among the\\niterative LLM augmentation methods for complex\\nqueries, single-step methods for simpler queries,\\nand even no-retrieval-augmented methods for the\\nmost straightforward queries (answerable by LLMs\\nthemselves), thus significantly enhancing the over-\\nall efficiency and accuracy, as shown in Figure 1.\\nWe refer to our framework as Adaptive Retrieval-\\nAugmented Generation (Adaptive-RAG).\\nWe validate Adaptive-RAG using benchmark\\nopen-domain QA datasets, covering a wide range\\nof query complexity from single-hop (Rajpurkar\\net al., 2016; Joshi et al., 2017; Kwiatkowski et al.,\\n2019) to multi-hop (Yang et al., 2018; Ho et al.,\\n2020; Trivedi et al., 2022b) queries. The exper-\\nimental results show that ours significantly im-\\nproves the overall accuracy and efficiency, com-\\npared to the prior adaptive strategies, on multiple\\nLLMs, such as GPT-3.5 (Brown et al., 2020) and\\nFLAN-T5 series (Chung et al., 2022).'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 2}, page_content='Our contributions and findings are threefold:\\n•We point out the realistic scenario of queries of\\nvarying complexities, and find out that existing\\nretrieval-augmented generation approaches tend\\nto be overly simple or complex.\\n•We adapt retrieval-augmented LLMs to the query\\ncomplexity assessed by the classifier, which en-\\nables the utilization of the most suitable approach\\ntailored to each query.\\n•We show that our Adaptive-RAG is highly effec-\\ntive and efficient, balancing between the com-\\nplexity and the simplicity for diverse queries.\\n2 Related Work\\nOpen-domain QA Open-domain QA is the task\\nof accurately answering a query by sourcing for\\nquery-relevant documents, and then interpreting\\nthem to provide answers (Chen et al., 2017; Zhu\\net al., 2021), which, thus, generally involves two\\nmodules: a retriever (Karpukhin et al., 2020; Xiong\\net al., 2021) and a reader (Yang et al., 2019; Izac-\\nard and Grave, 2021; Jeong et al., 2023). Along\\nwith the emergence of LLMs with superior rea-\\nsoning capabilities thanks to their billion-sized pa-\\nrameters (Wei et al., 2022a), a synergy between\\nLLMs and retrievers has led to significant advance-\\nments (Lazaridou et al., 2022; Ram et al., 2023).\\nSpecifically, this integration has been shown to\\nenhance Open-domain QA by mitigating the hallu-\\ncination problem from LLMs through strengthened\\nreasoning abilities of the reader, as well as utiliz-\\ning the retrieved, external documents (Cho et al.,\\n2023). Despite these advancements for single-hop\\nretrieval-augmented LLMs, however, the complex-\\nity of some queries needs a more complex strategy.\\nMulti-hop QA Multi-hop QA is an extension of\\nconventional Open-domain QA, which addition-\\nally requires the system to comprehensively gather\\nand contextualize information from multiple docu-\\nments (often iteratively), to answer more complex\\nqueries (Trivedi et al., 2022a; Yang et al., 2018). In\\nthe realm of multi-hop QA, the approach to itera-\\ntively access both LLMs and the retrieval module\\nis generally employed. Specifically, Khattab et al.\\n(2022), Press et al. (2023), Pereira et al. (2023)\\nand Khot et al. (2023) proposed to first decom-\\npose the multi-hop queries into simpler single-hop\\nqueries, repeatedly access the LLMs and retriever\\nto solve these sub-queries, and merge their solu-\\ntions to formulate a complete answer. In contrastto this decomposition-based approach, other re-\\ncent studies, such as Yao et al. (2023) and Trivedi\\net al. (2023), explored the interleaving of Chain-of-\\nThought reasoning (Wei et al., 2022b) — a method\\nwhere a logical sequence of thoughts is generated\\n— with document retrieval, repeatedly applying this\\nprocess until the reasoning chain generates the an-\\nswer. In addition, Jiang et al. (2023) introduced an\\napproach to repeatedly retrieving new documents\\nif the tokens within generated sentences have low\\nconfidence. However, the aforementioned methods\\noverlooked the fact that, in real-world scenarios,\\nqueries are of a wide variety of complexities. There-\\nfore, it would be largely inefficient to iteratively\\naccess LLMs and retrievers for every query, which\\nmight be simple enough with a single retrieval step\\nor even only with an LLM itself.\\nAdaptive Retrieval To handle queries of varying\\ncomplexities, the adaptive retrieval strategy aims to\\ndynamically decide whether to retrieve documents\\nor not, based on each query’s complexity. In this\\nvein, Mallen et al. (2023) proposed to decide the\\nquery’s complexity level based on the frequency of\\nits entities and suggested using the retrieval mod-\\nules only when the frequency falls below a cer-\\ntain threshold. However, this approach, focusing\\nsolely on the binary decision of whether to retrieve\\nor not, may not be sufficient for more complex\\nqueries that require multiple reasoning steps. Ad-\\nditionally, Qi et al. (2021) proposed an approach\\nthat performs a fixed set of operations (retrieving,\\nreading, and reranking) multiple times until the an-\\nswer is derived for the given query, which is built\\nupon traditional BERT-like LMs. However, unlike\\nour Adaptive-RAG which pre-determines the query\\ncomplexity and adapts the operational behavior of\\nany off-the-shelf LLMs accordingly, this approach\\napplies the same fixed operations to every query\\nregardless of its complexity but also necessitates\\nadditional specific training to LMs. Concurrent to\\nour work, Asai et al. (2024) suggested training a so-\\nphisticated model to dynamically retrieve, critique,\\nand generate the text. Nevertheless, we argue that\\nall the aforementioned adaptive retrieval methods\\nthat rely on a single model might be suboptimal in\\nhandling a variety of queries of a range of differ-\\nent complexities since they tend to be either overly\\nsimple or complex for all the input queries, which\\ndemands a new approach that can select the most\\nsuitable strategy of retrieval-augmented LLMs tai-\\nlored to the query complexity.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 3}, page_content='3 Method\\nIn this section, we describe our approach to adapt-\\ning retrieval-augmented LLMs, by pre-determining\\nthe query complexity and then selecting the most\\nfitting strategies for retrieval-augmented LLMs.\\n3.1 Preliminaries\\nWe begin with preliminaries, formally introducing\\ndifferent strategies of retrieval-augmented LLMs.\\nNon Retrieval for QA Let us first define an LLM\\nas a model LLM, which takes a sequence of tokens\\nx= [x1, x2, ..., x n]as an input and then generates\\na sequence of tokens y= [y1, y2, ..., y n]as an out-\\nput, which is formalized as follows: y=LLM(x).\\nThen, in our problem setup for QA, xandybe-\\ncome the input query ( q) from the user and the\\ngenerated answer ( ¯a) from the LLM, respectively:\\nq=xand¯a=y. Also, subsequently, the most\\nnaïve LLM-powered QA model can be represented\\nas follows: ¯a=LLM(q). Ideally, ¯ashould match\\nthe actual correct answer a. This non-retrieval-\\nbased QA method is highly efficient and could be\\na somewhat promising approach to handling easy\\nqueries, as the size of LLMs becomes extremely\\nlarge with its effect on storing a large amount of\\nknowledge. However, this approach is largely prob-\\nlematic on queries that require precise or concur-\\nrent knowledge of specific people, events, or any\\nsubjects beyond the LLMs’ internal knowledge.\\nSingle-step Approach for QA To address the\\naforementioned scenarios where LLMmay struggle\\nwith queries that are not answerable by LLMitself,\\nwe can utilize the external knowledge d, which\\nincludes useful information for queries, retrieved\\nfrom the external knowledge source Dthat could\\nbe an encyclopedia (e.g., Wikipedia) consisting\\nof millions of documents. Specifically, to obtain\\nsuchdfromD, a specific retrieval model is nec-\\nessary, which returns documents based on their\\nrelevance with the given query. This process can\\nbe formulated as follows: d=Retriever (q;D),\\nwhere Retriever is the retrieval model, with\\nd∈ D . Here, we can use any off-the-shelf re-\\ntriever (Robertson et al., 1994; Karpukhin et al.,\\n2020).\\nAfter the retrieval step is done, we now have a\\npair of query qand its relevant documents d. Then,\\nin order to augment LLMs with this retrieved exter-\\nnal knowledge, we can incorporate it into the input\\nof LLMs, represented as follows: ¯a=LLM(q,d).This process allows LLMs to gain access to exter-\\nnal information contained in d, which can provide\\nthe supplementary context that the internal knowl-\\nedge of LLMlacks, which can subsequently improve\\nthe accuracy and concurrency of LLMs for QA.\\nMulti-step Approach for QA Even though the\\naforementioned single-step approach offers signif-\\nicant improvements over non-retrieval for qthat\\nrequires external knowledge, it encounters notable\\nlimitations, particularly when dealing with com-\\nplex queries that necessitate synthesizing informa-\\ntion from multiple source documents and reasoning\\nover them. This is where a multi-step approach and\\nreasoning for QA become essential.\\nIn this multi-step approach, LLMinteracts with\\nRetriever in several rounds, progressively refin-\\ning its understanding of q, until it formulates the fi-\\nnal answer from findings accumulated across these\\nmultiple steps. Specifically, the process begins\\nwith the initial query q, and at every retrieval step\\ni, new documents diare retrieved from Dand then\\nincorporated into the input of LLMs, as follows:\\n¯ai=LLM(q,di,ci), where the additional context\\ncican be composed of previous documents and\\noutcomes (d1,d2, ...,di−1,¯a1,¯a2, ...,¯ai−1), and\\ndi=Retriever (q,ci;D)1. We would like to\\nnote that this iterative, multi-step process enables\\nLLMto construct a more comprehensive and exten-\\nsive foundation to solve queries effectively, specif-\\nically adept at complex multi-hop queries where\\nanswers depend on interconnected pieces of infor-\\nmation. However, it is important to recognize that\\nthis multi-step approach can be resource-intensive\\ndue to the repeated accesses to Retriever andLLM,\\nwhich entail substantial computational costs.\\n3.2 Adaptive-RAG: Adaptive\\nRetrieval-Augmented Generation\\nWe now introduce our adaptive retrieval-augmented\\nLLMs, which are built upon three different strate-\\ngies described in the previous section, and which\\nare designed to select the most suitable strategy\\naccording to the complexity of queries.\\nAdapting Retrieval-Augmented LLMs Note\\nthat in real-world scenarios, not all qfrom users\\nhave the same level of complexity, necessitating\\n1It is worth noting that implementations of the LLM and\\nretriever vary across different multi-step retrieval-augmented\\nLLM approaches (Trivedi et al., 2023; Press et al., 2023; Yao\\net al., 2023); therefore, the context cimay incorporate none,\\nsome, or all of the previous documents and answers.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 4}, page_content='tailored strategies for handling each query. In other\\nwords, employing the most basic, non-retrieval-\\nbased approach LLM(q)to respond to the complex\\nquery qwould be also ineffective (Figure 2, A);\\nconversely, using a more elaborate multi-step ap-\\nproach LLM(q,d,c)for simple qwould be ineffi-\\ncient (Figure 2, B). Therefore, our adaptive frame-\\nwork is designed to dynamically adjust the query-\\nhandling strategy of retrieval-augmented LLMs,\\nwhich is achieved by determining the complexity of\\neach query before attempting a solution. Notably,\\nthis framework can offer a robust middle ground\\nwith a range of solutions, from the simplest ap-\\nproach for the most straightforward queries, to the\\none-step approach for moderate queries, and up to\\nthe most comprehensive and rigorous approach for\\ncomplex queries. In addition, since the operations\\nofLLMandRetriever remain consistent regard-\\nless of inputs to them, our method can seeming-\\nlessly go back and forth across queries of different\\ncomplexities, without changing the internal model\\narchitecture or parameters during adaption.\\nQuery Complexity Assessment To operational-\\nize our adaptive retrieval-augmented LLM frame-\\nwork, we should determine the query complexity,\\nand to achieve this, we propose to model a com-\\nplexity classifier, whose goal is to return the appro-\\npriate complexity level of the given query. Specif-\\nically, given the query q, our classifier can be for-\\nmulated as follows: o=Classifier (q), where\\nClassifier is a smaller Language Model that is\\ntrained to classify one of three different complexity\\nlevels and ois its corresponding class label. In our\\nclassifier design, there are three class labels: ‘A’,\\n‘B’, and ‘C’, where ‘A’ indicates that qis straight-\\nforward and answerable by LLM(q)itself, ‘B’ in-\\ndicates that qhas the moderate complexity where\\nat least a single-step approach LLM(q,d)is needed,\\nand ‘C’ indicates that qis complex, requiring the\\nmost extensive solution LLM(q,d,c)2.\\nTraining Strategy The remaining step is to train\\nthe smaller Language Model for Classifier , to\\naccurately predict its complexity oin response to\\nthe given query q. Yet, there is no annotated dataset\\navailable for query-complexity pairs. Hence, we\\npropose to automatically construct the training\\ndataset with two particular strategies.\\nTo be specific, we first aim at labeling the query\\n2We consider three levels of query complexity, and leave\\nthe exploration of more fine-grained complexities as future\\nwork.complexity based on the results from three different\\nretrieval-augmented LLM strategies, in order to\\ndetermine the label by its needs. For example, if\\nthe simplest non-retrieval-based approach correctly\\ngenerates the answer, the label for its corresponding\\nquery is assigned ‘A’. Also, to break the tie between\\ndifferent models in providing the label to the query,\\nwe provide a higher priority to a simpler model.\\nIn other words, if both single-step and multi-step\\napproaches produce the same correct answer while\\nthe non-retrieval-based approach fails, we assign\\nlabel ‘B’ to its corresponding query.\\nHowever, this labeling strategy has a limita-\\ntion in that not all the queries are assigned labels,\\nsince the three retrieval-augmented approaches\\nmay all fail to generate the correct answer. On\\nthe other hand, the benchmark datasets may al-\\nready have meaningful inductive biases about the\\nmost appropriate retrieval-augmented LLM strate-\\ngies for their queries, considering the ways they\\nare created (e.g., QA datasets that require sequen-\\ntial reasoning usually necessitate a multi-step ap-\\nproach; while queries of those with labeled sin-\\ngle documents can be ideally answerable with the\\nsingle-step approach). Therefore, for those queries\\nthat remain unlabeled after the first labeling step,\\nwe assign ‘B’ to queries in single-hop datasets\\nand ‘C’ to queries in multi-hop datasets. Finally,\\nwe train Classifier with these automatically-\\ncollected query-complexity pairs3, by using a cross-\\nentropy loss. Then, at inference, we can deter-\\nmine the complexity of the query, which is one of\\n{‘A’, ‘B’, ‘C’}, by forwarding it to Classifier :\\no=Classifier (q).\\n4 Experimental Setups\\nIn this section, we explain datasets, models, met-\\nrics, and implementation details. We provide addi-\\ntional details in Appendix A.\\n4.1 Datasets\\nIn order to simulate a realistic scenario, where dif-\\nferent queries have varying complexities, we use\\nboth the single-hop and multi-hop QA datasets si-\\nmultaneously, in the unified experimental setting.\\nSingle-hop QA For simpler queries, we use three\\nbenchmark single-hop QA datasets, which consist\\n3As we automatically assign classifier labels, there might\\nbe errors in labeling and might be more advanced strategies to\\nautomatically assign labels, which we leave as future work.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 5}, page_content='Table 1: Averaged results on a collection of benchmark datasets for open-domain question answering including the single-hop\\nand multi-hop queries, with different LLMs. Self-RAG∗is trained with a different base LLM, namely LLaMA2 (Touvron et al.,\\n2023); therefore, we compare the results of FLAN-T5-XL (3B) with the results from Self-RAG with LLaMA2 (7B) and the\\nresults of others with the results from Self-RAG with LLaMA2 (13B). We emphasize our results in bold, for easy comparisons.\\nFLAN-T5-XL (3B) FLAN-T5-XXL (11B) GPT-3.5 (Turbo)\\nTypes Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nSimpleNo Retrieval 14.87 21.12 15.97 0.00 0.11 17.83 25.14 19.33 0.00 0.08 35.77 48.56 44.27 0.00 0.71\\nSingle-step Approach 34.83 44.31 38.87 1.00 1.00 37.87 47.63 41.90 1.00 1.00 34.73 46.99 45.27 1.00 1.00\\nAdaptiveAdaptive Retrieval 23.87 32.24 26.73 0.50 0.56 26.93 35.67 29.73 0.50 0.54 35.90 48.20 45.30 0.50 0.86\\nSelf-RAG∗9.90 20.79 31.57 0.72 0.43 10.87 22.98 34.13 0.74 0.23 10.87 22.98 34.13 0.74 1.50\\nAdaptive-RAG (Ours) 37.17 46.94 42.10 2.17 3.60 38.90 48.62 43.77 1.35 2.00 37.97 50.91 48.97 1.03 1.46\\nComplex Multi-step Approach 39.00 48.85 43.70 4.69 8.81 40.13 50.09 45.20 2.13 3.80 38.13 50.87 49.70 2.81 3.33\\nOracle Adaptive-RAG w/ Oracle 45.00 56.28 49.90 1.28 2.11 47.17 58.60 52.20 0.84 1.10 47.70 62.80 58.57 0.50 1.03\\nof queries and their associated documents contain-\\ning answers, namely 1) SQuAD v1.1 (Rajpurkar\\net al., 2016), 2) Natural Questions (Kwiatkowski\\net al., 2019), and 3) TriviaQA (Joshi et al., 2017).\\nMulti-hop QA To consider more complex query\\nscenarios, we use three benchmark multi-hop QA\\ndatasets, which require sequential reasoning over\\nmultiple documents, namely 1) MuSiQue (Trivedi\\net al., 2022a), 2) HotpotQA (Yang et al., 2018),\\nand3) 2WikiMultiHopQA (Ho et al., 2020).\\n4.2 Models\\nWe compare our Adaptive-RAG against relevant\\nmodels, including three retrieval-augmented LLM\\nstrategies (in Section 3.1) and the adaptive re-\\ntrieval approaches (Mallen et al., 2023; Asai et al.,\\n2024), which can be grouped into one of three cat-\\negories: Simple, Adaptive, and Complex. Specif-\\nically, Simple approaches include the 1) No Re-\\ntrieval and2) Single-step Approach -based meth-\\nods. Adaptive approaches include the 3) Adaptive\\nRetrieval (Mallen et al., 2023), 4) Self-RAG (Asai\\net al., 2024), and our 5) Adaptive-RAG , which\\ncan adaptively perform retrieval based on the\\nquestion complexity. For the 6) Multi-step Ap-\\nproach , we use the most sophisticated state-of-\\nthe-art method (Trivedi et al., 2023), iteratively\\naccessing both the retriever and LLM with Chain-\\nof-Thought reasoning (Wei et al., 2022b), for every\\nquery. Note that models across different categories\\nare not directly comparable. Yet, in the ideal set-\\nting, Adaptive approaches should be more effective\\nthan those in the Simple category while simultane-\\nously being more efficient than the Complex one.\\nTherefore, we also report the performance in an\\nideal scenario, 7) Adaptive-RAG w/ Oracle , using\\nthe oracle classifier with our Adaptive-RAG.\\n4.3 Evaluation Metrics\\nWhen it comes to evaluating adaptive models, it\\nis essential to simultaneously consider both thetask performance and efficiency along with their\\ntrade-offs. Thus, we report the results with five\\nmetrics, where three of them measure the effective-\\nness and the other two measure the efficiency. In\\nparticular, for effectiveness, we use F1, EM, and\\nAccuracy (Acc), following the standard evaluation\\nprotocol (Mallen et al., 2023; Baek et al., 2023;\\nAsai et al., 2024), where F1 measures the number\\nof overlapping words between the predicted an-\\nswer and the ground truth, EM measures whether\\nthey are the same, and Acc measures whether the\\npredicted answer contains the ground-truth answer.\\nFor efficiency, we measure the number of retrieval-\\nand-generate steps and the average time for answer-\\ning each query relative to the one-step approach.\\n4.4 Implementation Details\\nFor a fair comparison and following Mallen et al.\\n(2023) and Trivedi et al. (2023), we use the same re-\\ntriever, a term-based sparse retrieval model known\\nas BM25 (Robertson et al., 1994), across all differ-\\nent models. For the external document corpus, we\\nuse different sources depending on the dataset type:\\nthe Wikipedia corpus preprocessed by Karpukhin\\net al. (2020) for single-hop datasets, and the pre-\\nprocessed corpus by Trivedi et al. (2023) for multi-\\nhop datasets. Regarding the LLMs that are used\\nto generate answers, we use the FLAN-T5 series\\nmodels (Chung et al., 2022) of XL with 3B pa-\\nrameters and XXL with 11B parameters, and the\\nGPT-3.5 model (gpt-3.5-turbo-instruct). For the\\nretrieval-augmented LLM design, we follow the\\nimplementation details from Trivedi et al. (2023),\\nwhich include input prompts, instructions, and the\\nnumber of test samples for evaluation (e.g., 500\\nsamples per dataset). In our Adaptive-RAG, for the\\nquery-complexity classifier, we use and train the\\nT5-Large model (Raffel et al., 2020). Specifically,\\nthe classifier is trained using the epoch that shows\\nthe best performance until 100 training iterations\\nfrom the validation set, with the learning rate of 3e-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='Table 2: Results on each of a collection of datasets with FLAN-T5-XL (3B) as the LLM. We emphasize our results in bold.\\nSQuAD Natural Questions TriviaQA\\nData Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nSingle-stepSimpleNo Retrieval 3.60 10.50 5.00 0.00 0.11 14.20 19.00 15.60 0.00 0.13 25.00 31.80 27.00 0.00 0.13\\nSingle-step Approach 27.80 39.30 34.00 1.00 1.00 37.80 47.30 44.60 1.00 1.00 53.60 62.40 60.20 1.00 1.00\\nAdaptiveAdaptive Retrieval 13.40 23.10 17.60 0.50 0.55 28.20 36.00 33.00 0.50 0.56 38.40 46.90 42.60 0.50 0.56\\nSelf-RAG∗2.20 11.20 18.40 0.63 0.50 31.40 39.00 33.60 0.63 0.17 12.80 29.30 57.00 0.68 0.45\\nAdaptive-RAG (Ours) 26.80 38.30 33.00 1.37 2.02 37.80 47.30 44.60 1.00 1.00 52.20 60.70 58.20 1.23 1.54\\nComplex Multi-step Approach 24.40 35.60 29.60 4.52 9.03 38.60 47.80 44.20 5.04 10.18 53.80 62.40 60.20 5.28 9.22\\nOracle Adaptive-RAG w/ Oracle 32.00 45.60 38.20 1.24 1.60 47.40 57.10 53.60 1.10 1.55 61.60 70.20 66.40 0.79 1.10\\nMuSiQue HotpotQA 2WikiMultiHopQA\\nData Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nMulti-stepSimpleNo Retrieval 2.40 10.70 3.20 0.00 0.11 16.60 22.71 17.20 0.00 0.11 27.40 32.04 27.80 0.00 0.10\\nSingle-step Approach 13.80 22.80 15.20 1.00 1.00 34.40 46.15 36.40 1.00 1.00 41.60 47.90 42.80 1.00 1.00\\nAdaptiveAdaptive Retrieval 6.40 15.80 8.00 0.50 0.55 23.60 32.22 25.00 0.50 0.55 33.20 39.44 34.20 0.50 0.55\\nSelf-RAG∗1.60 8.10 12.00 0.73 0.51 6.80 17.53 29.60 0.73 0.45 4.60 19.59 38.80 0.93 0.49\\nAdaptive-RAG (Ours) 23.60 31.80 26.00 3.22 6.61 42.00 53.82 44.40 3.55 5.99 40.60 49.75 46.40 2.63 4.68\\nComplex Multi-step Approach 23.00 31.90 25.80 3.60 7.58 44.60 56.54 47.00 5.53 9.38 49.60 58.85 55.40 4.17 7.37\\nOracle Adaptive-RAG w/ Oracle 24.80 38.50 27.00 1.98 3.99 51.20 64.00 54.80 1.59 2.77 53.00 62.30 59.40 1.01 1.69\\nF1102030405060Adaptive Retrieval\\nSelf-RAG\\nAdaptive-RAG (Ours)\\nClassifier Acc.3540455055FLAN-T5-XL\\nF1102030405060Adaptive Retrieval\\nSelf-RAG\\nAdaptive-RAG (Ours)\\nClassifier Acc.3540455055FLAN-T5-XXL\\nNo One MultiNo One Multi0.31 0.47 0.22\\n0.1 0.66 0.23\\n0.03 0.31 0.65\\n0.20.40.6Confusion Matrix\\nFigure 3: Performance on QA and query-complexity assessment of different adaptive approaches for retrieval-augmented LLMs\\nwith FLAN-T5 XL (Left) and XXL (Center). For labeling the complexity of queries, we use the silver data annotated from the\\nprediction outcomes of models (described in Section 3.2). We also provide the confusion matrix across three labels (Right).\\n5 and the AdamW (Loshchilov and Hutter, 2019)\\nas an optimizer. Regarding its training data, we\\nsample and annotate 400 queries from 6 datasets\\nbased on its inductive bias (single-hop for one-step\\napproach and multi-hop for multi-step). In addition,\\nwe use predicted outcomes of three different strate-\\ngies over 400 queries sampled from each dataset.\\nNote that those queries used for classifier training\\ndo not overlap with the testing queries for QA.\\n5 Experimental Results and Analyses\\nIn this section, we show the overall experimental\\nresults and offer in-depth analyses of our method.\\nMain Results First of all, Table 1 shows our main\\nresults averaged over all considered datasets, which\\ncorroborate our hypothesis that simple retrieval-\\naugmented strategies are less effective than the\\ncomplex strategy, while the complex one is sig-\\nnificantly more expensive than the simple ones. In\\naddition, we report the more granular results with\\nFLAN-T5-XL on each of the single-hop and multi-\\nhop datasets in Table 2 (and more with different\\nLLMs in Table 7 and Table 8 of Appendix), which\\nare consistent with the results observed in Table 1.\\nHowever, in a real-world scenario, not all users\\nask queries with the same level of complexity,\\nwhich emphasizes the importance of the need for\\nadaptive strategies. Note that among the adaptive\\nstrategies, our Adaptive-RAG shows remarkableeffectiveness over the competitors (Table 1). This\\nindicates that merely focusing on the decision of\\nwhether to retrieve or not is suboptimal. Also, as\\nshown in Table 2, such simple adaptive strategies\\nare particularly inadequate for handling complex\\nqueries in multi-hop datasets, which require ag-\\ngregated information and reasoning over multiple\\ndocuments. Meanwhile, our approach can consider\\na more fine-grained query handling strategy by fur-\\nther incorporating an iterative module for complex\\nqueries. Furthermore, in a realistic setting, we\\nshould take into account not only effectiveness but\\nalso efficiency. As shown in Table 1, compared to\\nthe complex multi-step strategy, our proposed adap-\\ntive strategy is significantly more efficient across\\nall model sizes. This is meaningful in this era of\\nLLMs, where the cost of accessing them is a critical\\nfactor for practical applications and scalability. Fi-\\nnally, to see the upper bound of our Adaptive-RAG,\\nwe report its performances with the oracle classifier\\nwhere the classification performance is perfect. As\\nshown in Table 1 and Table 2, we observe that it\\nachieves the best performance while being much\\nmore efficient than our Adaptive-RAG without the\\noracle classifier. These results support the valid-\\nity and significance of our proposal for adapting\\nretrieval-augmented LLM strategies based on query\\ncomplexity, and further suggest the direction to de-\\nvelop more improved classifiers to achieve optimal\\nperformance.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 7}, page_content='Table 3: The exact elapsed time per query and the percentage\\nof the predicted labels from the classifier over all samples.\\nLabels Time/Query (Sec.) Percentage (%)\\nNo (A) 0.35 8.60\\nOne (B) 3.08 53.33\\nMulti (C) 27.18 38.07\\nClassifier Performance To understand how the\\nproposed classifier works, we analyze its perfor-\\nmance across different complexity labels. As Fig-\\nure 3 (Left and Center) shows, the classification\\naccuracy of our Adaptive-RAG is better than those\\nof the other adaptive retrieval baselines, which\\nleads to overall QA performance improvements. In\\nother words, this result indicates that our Adaptive-\\nRAG is capable of more accurately classifying the\\ncomplexity levels with various granularities, which\\ninclude not performing retrieval, performing re-\\ntrieval only once, and performing retrieval multiple\\ntimes. In addition to the true positive performance\\nof our classifier averaged over all those three la-\\nbels in Figure 3 (Left and Center), we further re-\\nport its confusion matrix in Figure 3 (Right). We\\nnote that the confusion matrix reveals some notable\\ntrends: ‘C (Multi)’ is sometimes misclassified as\\n‘B (One)’ (about 31%) and ‘B (One)’ as ‘C (Multi)’\\n(about 23%); ‘A (No)’ is misclassified often as\\n‘B (One)’ (about 47%) and less frequently as ‘C\\n(Multi)’ (about 22%). While the overall results in\\nFigure 3 show that our classifier effectively cate-\\ngorizes the three labels, further refining it based\\non such misclassification would be a meaningful\\ndirection for future work.\\nAnalyses on Efficiency for Classifier While Ta-\\nble 1 shows the relative elapsed time for each of the\\nthree different RAG strategies, we further provide\\nthe exact elapsed time per query for our Adaptive-\\nRAG and the distribution for predicted labels from\\nour query-complexity classifier in Table 3. Similar\\nto the results of the elapsed time in Table 1 (relative\\ntime), Table 3 (exact time) shows that efficiency\\ncan be substantially improved by identifying sim-\\nple or straightforward queries.\\nAnalyses on Training Data for Classifier We\\nhave shown that the classifier plays an important\\nrole in adaptive retrieval. Here, we further analyze\\nthe different strategies for training the classifier by\\nablating our full training strategy, which includes\\ntwo approaches: generating silver data from pre-\\ndicted outcomes of models and utilizing inductiveTable 4: Results on QA and complexity classification with\\nvarying the data annotation strategies for training the classifier.\\nQA Classifier (Accuracy)\\nTraining Strategies F1 Step All No One Multi\\nAdaptive-RAG (Ours) 46.94 1084 54.52 30.52 66.28 65.45\\nw/o Binary 43.43 640 60.30 62.19 65.70 39.55\\nw/o Silver 48.79 1464 40.00 0.00 53.98 75.91\\nbias in datasets (see Section 3.2). As Table 4 shows,\\ncompared to the training strategy relying solely on\\nthe data derived from inductive bias, ours is sig-\\nnificantly more efficient. This efficiency is partly\\nbecause ours also takes into account the case that\\ndoes not consider any documents at all, as also\\nimplied by the classification accuracy; meanwhile,\\nqueries in the existing datasets do not capture the\\ninformation on whether the retrieval is required or\\nnot. On the other hand, in the case of only using the\\nsilver data annotated from the correct predictions,\\nwhile its overall classification accuracy is high, the\\noverall QA performance implies that relying on\\nthe silver data may not be optimal. This may be\\nbecause this silver data does not cover complex-\\nity labels over incorrectly predicted queries, which\\nleads to lower generalization effect on queries rel-\\nevant to them. Meanwhile, by also incorporating\\ncomplexity labels from dataset bias (single-hop vs\\nmulti-hop), the classifier becomes more accurate in\\npredicting multi-hop queries, leading to the better\\nperformance. It is worth noting that our automatic\\nlabeling strategies are two particular instantiations\\nfor training the classifier, and that there could be\\nother instantiations, which we leave as future work.\\nAnalyses on Classifier Size To investigate the\\nsensitivity of our classifier according to its varying\\nsizes, we conducted further experiments. As shown\\nin Table 6, we observe no significant performance\\ndifferences among classifiers of various sizes, even\\nwith reduced complexity and fewer parameters in\\nsmaller classifiers. This indicates that our proposed\\nclassifier can contribute to resource-efficient set-\\ntings in real-use cases with smaller sizes without\\ncompromising the performance.\\nCase Study We conduct a case study to qualita-\\ntively compare our Adaptive-RAG against Adap-\\ntive Retrieval. Table 5 shows the classified com-\\nplexity and the query handling patterns for both\\nsimple and complex questions. First, for the simple\\nsingle-hop question, our Adaptive-RAG identifies\\nthat it is answerable by only using the LLM’s para-\\nmetric knowledge about ‘Google’. By contrast,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 8}, page_content='Table 5: Case study with GPT-3.5 (Turbo), where we represent the factual error in red and the accurate information in blue.\\nDataset Question Adaptive Retrieval Adaptive-RAG (Ours)\\nNQ\\n(Single-hop)Which famous corpo-\\nrate logo changed to a\\nflat colour/color sans\\nserif font in its first ma-\\njor change since 1999?Query Type: B (Single-step Approach)\\nReasoning: The famous corporate logo that\\nchanged to a flat color sans-serif font in its first\\nmajor change since 1999 is Microsoft. Microsoft\\nchanged its logo in 2012, replacing the italicized,\\nmulti-colored font with a simple, flat, grey font. So\\nthe answer is: Microsoft.Query Type: A (Non Retrieval)\\nReasoning: The famous corporate logo that\\nchanged to a flat color sans serif font in its first ma-\\njor change since 1999 is Google. Google changed\\nits logo in 2015. So the answer is: Google.\\nMuSiQue\\n(Multi-hop)Who is the child of the\\nItalian navigator who\\nexplored the eastern\\ncoast of the continent\\nCésar Gaytan was born\\nin for the English?Query Type: A (Non Retrieval)\\nReasoning: César Gaytan was born in the conti-\\nnent of North America. The Italian navigator who\\nexplored the eastern coast of North America for the\\nEnglish was Giovanni Caboto, also known as John\\nCabot. So the answer is: Giovanni Caboto/John\\nCabot.Query Type: C (Multi-step Approach)\\nReasoning: Césarr Gaytan was born in Guadalajara,\\nJalisco, Mexico. The Italian navigator who explored\\nthe eastern coast of the continent for the English is\\nJohn Cabot. John Cabot’s son is Sebastian Cabot.\\nSo the answer is: Sebastian Cabot.\\nTable 6: Results with varying model sizes for classifiers.\\nQA Classifier (Accuracy)\\nSizes F1 Step All No One Multi\\nSmall (60M) 45.83 964 53.48 26.65 70.62 53.18\\nBase (223M) 45.97 983 53.41 26.42 69.46 56.82\\nLarge (770M) 46.94 1084 54.52 30.52 66.28 65.45\\nAdaptive Retrieval fetches additional documents,\\nleading to longer processing times and occasion-\\nally producing incorrect responses due to the inclu-\\nsion of partially irrelevant information about ‘Mi-\\ncrosoft’. Meanwhile, faced with a complex ques-\\ntion, Adaptive-RAG seeks out relevant information,\\nincluding details like ‘a son of John Cabot’, which\\nmay not have been stored in LLMs, while Adaptive\\nRetrieval fails to request such information from\\nexternal sources, resulting in inaccurate answers.\\n6 Conclusion\\nIn this work, we proposed the Adaptive Retrieval-\\nAugmented Generation framework, referred to\\nas Adaptive-RAG, to handle queries of various\\ncomplexities. Specifically, Adaptive-RAG is de-\\nsigned to dynamically adjust its query handling\\nstrategies in the unified retrieval-augmented LLM\\nbased on the complexity of queries that they en-\\ncounter, which spans across a spectrum of the non-\\nretrieval-based approach for the most straightfor-\\nward queries, to the single-step approach for the\\nqueries of moderate complexity, and finally to the\\nmulti-step approach for the complex queries. The\\ncore step of our Adaptive-RAG lies in determin-\\ning the complexity of the given query, which is\\ninstrumental in selecting the most suitable strat-\\negy for its answer. To operationalize this process,\\nwe trained a smaller Language Model with query-\\ncomplexity pairs, which are automatically anno-\\ntated from the predicted outcomes and the inductive\\nbiases in datasets. We validated our Adaptive-RAGon a collection of open-domain QA datasets, cover-\\ning the multiple query complexities including both\\nthe single- and multi-hop questions. The results\\ndemonstrate that our Adaptive-RAG enhances the\\noverall accuracy and efficiency of QA systems, al-\\nlocating more resources to handle complex queries\\nwhile efficiently handling simpler queries, com-\\npared to the existing one-size-fits-all approaches\\nthat tend to be either minimalist or maximalist over\\nvarying query complexities.\\nLimitations\\nWhile our Adaptive-RAG shows clear advantages\\nin effectiveness and efficiency by determining the\\nquery complexity and then leveraging the most\\nsuitable approach for tackling it, it is important\\nto recognize that there still exist potential avenues\\nfor improving the classifier from the perspectives\\nof its training datasets and architecture. Specifi-\\ncally, as there are no available datasets for training\\nthe query-complexity classifier, we automatically\\ncreate new data based on the model prediction out-\\ncomes and the inductive dataset biases. However,\\nour labeling process is one specific instantiation\\nof labeling the query complexity, and it may have\\nthe potential to label queries incorrectly despite its\\neffectiveness. Therefore, future work may create\\nnew datasets that are annotated with a diverse range\\nof query complexities, in addition to the labels of\\nquestion-answer pairs. Also, as the performance\\ngap between the ideal classifier in Table 1 and the\\ncurrent classifier in Figure 3 indicates, there is still\\nroom to improve the effectiveness of the classifier.\\nIn other words, our classifier design based on the\\nsmaller LM is the initial, simplest instantiation for\\nclassifying the query complexity, and based upon\\nit, future work may improve the classifier archi-\\ntecture and its performance, which will positively\\ncontribute to the overall QA performance.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 9}, page_content='Ethics Statement\\nThe experimental results on Adaptive-RAG vali-\\ndate its applicability in realistic scenarios, where a\\nwide range of diverse user queries exist. Nonethe-\\nless, given the potential diversity of real-world user\\ninputs, it is crucial to also consider scenarios where\\nthese inputs might be offensive or harmful. We\\nshould be aware that such inputs could lead to the\\nretrieval of offensive documents and the genera-\\ntion of inappropriate responses by the retrieval-\\naugmented LLMs. To address this challenge, de-\\nveloping methods to detect and manage offensive\\nor inappropriate content in both user inputs and re-\\ntrieved documents within the retrieval-augmented\\nframework is essential. We believe that this is a\\ncritical area for future work.\\nAcknowledgements\\nThis work was supported by Institute for Informa-\\ntion and communications Technology Promotion\\n(IITP) grant funded by the Korea government (No.\\n2018-0-00582, Prediction and augmentation of the\\ncredibility distribution via linguistic analysis and\\nautomated evidence document collection), Basic\\nScience Research Program through the National\\nResearch Foundation of Korea (NRF) funded by the\\nMinistry of Education (RS-2023-00275747), and\\nthe Artificial intelligence industrial convergence\\ncluster development project funded by the Ministry\\nof Science and ICT (MSIT, Korea) & Gwangju\\nMetropolitan City.\\nReferences\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\\nChen, Eric Chu, Jonathan H. Clark, Laurent El\\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\\nYuanzhong Xu, Yujing Zhang, Gustavo Hernández\\nÁbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\\nJan A. Botha, James Bradbury, Siddhartha Brahma,\\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\\nCherry, Christopher A. Choquette-Choo, Aakanksha\\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\\nNan Du, Ethan Dyer, Vladimir Feinberg, Fangxi-\\naoyu Feng, Vlad Fienber, Markus Freitag, Xavier\\nGarcia, Sebastian Gehrmann, Lucas Gonzalez, and\\net al. 2023. Palm 2 technical report. arXiv preprint\\narXiv:2305.10403 .\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, andHannaneh Hajishirzi. 2024. Self-RAG: Learning to\\nretrieve, generate, and critique through self-reflection.\\nInThe Twelfth International Conference on Learning\\nRepresentations .\\nJinheon Baek, Soyeong Jeong, Minki Kang, Jong Park,\\nand Sung Ju Hwang. 2023. Knowledge-augmented\\nlanguage model verification. In Proceedings of the\\n2023 Conference on Empirical Methods in Natural\\nLanguage Processing, EMNLP 2023, Singapore, De-\\ncember 6-10, 2023 , pages 1720–1736. Association\\nfor Computational Linguistics.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\\n2022. Improving language models by retrieving from\\ntrillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA , volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240.\\nPMLR.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In Ad-\\nvances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December 6-12,\\n2020, virtual .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, ACL 2017, Vancouver, Canada, July 30 -\\nAugust 4, Volume 1: Long Papers , pages 1870–1879.\\nAssociation for Computational Linguistics.\\nSukmin Cho, Jeongyeon Seo, Soyeong Jeong, and\\nJong C. Park. 2023. Improving zero-shot reader by\\nreducing distractions from irrelevant documents in\\nopen-domain question answering. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2023, Singapore, December 6-10, 2023 , pages 3145–\\n3157. Association for Computational Linguistics.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 10}, page_content='Narang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\\n2022. Scaling instruction-finetuned language models.\\narXiv preprint arXiv:2210.11416 .\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\\nand Akiko Aizawa. 2020. Constructing A multi-hop\\nQA dataset for comprehensive evaluation of reason-\\ning steps. In Proceedings of the 28th International\\nConference on Computational Linguistics, COLING\\n2020, Barcelona, Spain (Online), December 8-13,\\n2020 , pages 6609–6625. International Committee on\\nComputational Linguistics.\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open do-\\nmain question answering. In Proceedings of the 16th\\nConference of the European Chapter of the Associ-\\nation for Computational Linguistics: Main Volume,\\nEACL 2021, Online, April 19 - 23, 2021 , pages 874–\\n880. Association for Computational Linguistics.\\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\\nEdouard Grave. 2023. Atlas: Few-shot learning\\nwith retrieval augmented language models. J. Mach.\\nLearn. Res. , 24:251:1–251:43.\\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\\nHwang, and Jong Park. 2023. Test-time self-adaptive\\nsmall language models for question answering. In\\nFindings of the Association for Computational Lin-\\nguistics: EMNLP 2023, Singapore, December 6-10,\\n2023 , pages 15459–15469. Association for Computa-\\ntional Linguistics.\\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\\nCallan, and Graham Neubig. 2023. Active retrieval\\naugmented generation. In EMNLP 2023 .\\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics, ACL\\n2017, Vancouver, Canada, July 30 - August 4, Volume\\n1: Long Papers , pages 1601–1611. Association for\\nComputational Linguistics.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\\nand Wen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Natu-\\nral Language Processing, EMNLP 2020, November\\n16-20, 2020 . Association for Computational Linguis-\\ntics.\\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ro-\\nnan Le Bras, Akari Asai, Xinyan Yu, Dragomir R.\\nRadev, Noah A. Smith, Yejin Choi, and Kentaro Inui.2022. Realtime QA: what’s the answer right now?\\narXiv preprint arXiv:2207.13332 .\\nOmar Khattab, Keshav Santhanam, Xiang Lisa\\nLi, David Hall, Percy Liang, Christopher Potts,\\nand Matei Zaharia. 2022. Demonstrate-search-\\npredict: Composing retrieval and language mod-\\nels for knowledge-intensive NLP. arXiv preprint\\narXiv.2212.14024 , abs/2212.14024.\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\\nharwal. 2023. Decomposed prompting: A modular\\napproach for solving complex tasks. In The Eleventh\\nInternational Conference on Learning Representa-\\ntions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 .\\nOpenReview.net.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, Kristina Toutanova, Llion Jones, Matthew\\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: A benchmark for question answering\\nresearch. Transactions of the Association for Compu-\\ntational Linguistics , 7:452–466.\\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\\nStokowiec, and Nikolai Grigorev. 2022. Internet-\\naugmented language models through few-shot\\nprompting for open-domain question answering.\\narXiv preprint arXiv:2203.05115 .\\nBelinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar\\nMehdad, and Wen-tau Yih. 2020. Efficient one-pass\\nend-to-end entity linking for questions. In Proceed-\\nings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing, EMNLP 2020, Online,\\nNovember 16-20, 2020 , pages 6433–6441. Associa-\\ntion for Computational Linguistics.\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\\nweight decay regularization. In 7th International\\nConference on Learning Representations, ICLR 2019,\\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\\nview.net.\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\\nWhen not to trust language models: Investigating\\neffectiveness of parametric and non-parametric mem-\\nories. In Proceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), ACL 2023, Toronto, Canada,\\nJuly 9-14, 2023 , pages 9802–9822. Association for\\nComputational Linguistics.\\nOpenAI. 2023. GPT-4 technical report. arXiv preprint\\narXiv:2303.08774 .\\nAdam Paszke, Sam Gross, Francisco Massa, Adam\\nLerer, James Bradbury, Gregory Chanan, Trevor\\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\\nAntiga, Alban Desmaison, Andreas Köpf, Edward Z.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='Yang, Zachary DeVito, Martin Raison, Alykhan Te-\\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\\nimperative style, high-performance deep learning li-\\nbrary. In Advances in Neural Information Processing\\nSystems 32: Annual Conference on Neural Informa-\\ntion Processing Systems 2019 , pages 8024–8035.\\nJayr Alencar Pereira, Robson do Nascimento Fidalgo,\\nRoberto de Alencar Lotufo, and Rodrigo Frassetto\\nNogueira. 2023. Visconde: Multi-document QA with\\nGPT-3 and neural reranking. In Advances in Informa-\\ntion Retrieval - 45th European Conference on Infor-\\nmation Retrieval, ECIR 2023, Dublin, Ireland, April\\n2-6, 2023, Proceedings, Part II , volume 13981 of\\nLecture Notes in Computer Science , pages 534–543.\\nSpringer.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A. Smith, and Mike Lewis. 2023. Measuring\\nand narrowing the compositionality gap in language\\nmodels. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2023 .\\nPeng Qi, Haejun Lee, Tg Sido, and Christopher D. Man-\\nning. 2021. Answering open-domain questions of\\nvarying reasoning steps from text. In Proceedings\\nof the 2021 Conference on Empirical Methods in\\nNatural Language Processing, EMNLP 2021, Vir-\\ntual Event / Punta Cana, Dominican Republic, 7-11\\nNovember, 2021 , pages 3599–3614. Association for\\nComputational Linguistics.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. J. Mach. Learn. Res. , 21:140:1–140:67.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. Squad: 100, 000+ questions\\nfor machine comprehension of text. In Proceedings\\nof the 2016 Conference on Empirical Methods in\\nNatural Language Processing, EMNLP 2016, Austin,\\nTexas, USA, November 1-4, 2016 , pages 2383–2392.\\nThe Association for Computational Linguistics.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. Transactions of the Association for\\nComputational Linguistics .\\nStephen E. Robertson, Steve Walker, Susan Jones,\\nMicheline Hancock-Beaulieu, and Mike Gatford.\\n1994. Okapi at TREC-3. In Proceedings of The Third\\nText REtrieval Conference, TREC 1994, Gaithers-\\nburg, Maryland, USA, November 2-4, 1994 , volume\\n500-225 of NIST Special Publication , pages 109–\\n126. National Institute of Standards and Technology\\n(NIST).\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\\nmoyer, and Wen-tau Yih. 2023. REPLUG: retrieval-augmented black-box language models. arXiv\\npreprint arXiv:2301.12652 .\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aurélien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. 2023. Llama 2: Open foundation and fine-\\ntuned chat models. arXiv preprint arXiv:2307.09288 .\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2022a. Musique: Multi-\\nhop questions via single-hop question composition.\\nTrans. Assoc. Comput. Linguistics , 10:539–554.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2022b. ♪MuSiQue: Multi-\\nhop questions via single-hop question composition.\\nTransactions of the Association for Computational\\nLinguistics , 10:539–554.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2023. Interleaving retrieval\\nwith chain-of-thought reasoning for knowledge-\\nintensive multi-step questions. In Proceedings of\\nthe 61st Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers),\\nACL 2023, Toronto, Canada, July 9-14, 2023 , pages\\n10014–10037. Association for Computational Lin-\\nguistics.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\\ngent abilities of large language models. Trans. Mach.\\nLearn. Res. , 2022.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\\nand Denny Zhou. 2022b. Chain-of-thought prompt-\\ning elicits reasoning in large language models. In\\nNeurIPS .\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 12}, page_content='Joe Davison, Sam Shleifer, Patrick von Platen, Clara\\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\\nScao, Sylvain Gugger, Mariama Drame, Quentin\\nLhoest, and Alexander M. Rush. 2020. Transform-\\ners: State-of-the-art natural language processing. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing: System\\nDemonstrations, EMNLP 2020 - Demos , pages 38–\\n45. Association for Computational Linguistics.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2021. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval. In 9th International Conference on Learning\\nRepresentations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021 . OpenReview.net.\\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\\nEnd-to-end open-domain question answering with\\nbertserini. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\\nUSA, June 2-7, 2019, Demonstrations , pages 72–77.\\nAssociation for Computational Linguistics.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainable multi-hop question answering.\\nInProceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n2369–2380, Brussels, Belgium. Association for Com-\\nputational Linguistics.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik R. Narasimhan, and Yuan Cao. 2023.\\nReact: Synergizing reasoning and acting in language\\nmodels. In The Eleventh International Conference\\non Learning Representations, ICLR 2023, Kigali,\\nRwanda, May 1-5, 2023 . OpenReview.net.\\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming\\nZheng, Soujanya Poria, and Tat-Seng Chua. 2021.\\nRetrieving and reading: A comprehensive survey on\\nopen-domain question answering. arXiv preprint\\narXiv:2101.00774 .'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 13}, page_content='0 2 4 6 8 10\\nTime per Query20304050Performance (F1)  No Retrieval  Single-step Approach\\n  Adaptive Retrieval  Multi-step Approach   Adaptive-RAG (Ours)Performance vs Time with FLAN-T5-XLFigure 4: QA performance (F1) and efficiency (Time/Query)\\nfor different retrieval-augmented generation approaches. We\\nuse the FLAN-T5-XL (3B) as the base LLM.\\nA Additional Experimental Setups\\nA.1 Datasets\\nWe use publicly open datasets for both single-\\nhop and multi-hop QA datasets, referring to\\nas Karpukhin et al. (2020) and Trivedi et al. (2023),\\nrespectively. We describe the characteristics of\\neach dataset:\\n1) SQuAD v1.1 (Rajpurkar et al., 2016) is created\\nthrough a process where annotators write questions\\nbased on the documents they read.\\n2) Natural Questions (Kwiatkowski et al., 2019) is\\nconstructed by real user queries on Google Search.\\n3) TriviaQA (Joshi et al., 2017) comprises trivia\\nquestions sourced from various quiz websites.\\n4) MuSiQue (Trivedi et al., 2022a) is collected by\\ncompositing multiple single-hop queries, to form\\nqueries spanning 2-4 hops.\\n5) HotpotQA (Yang et al., 2018) is constructed by\\nhaving annotators create questions that link multi-\\nple Wikipedia articles.\\n6) 2WikiMultiHopQA (Ho et al., 2020) is derived\\nfrom Wikipedia and its associated knowledge graph\\npath, needing 2-hops.\\nA.2 Models\\nWe describe the details of models as follows:\\n1) No Retrieval. This approach uses only the LLM\\nitself, to generate the answer to the given query.\\n2) Single-step Approach. This approach first re-\\ntrieves the relevant knowledge with the given query\\nfrom the external knowledge sources and then aug-\\nments the LLM with this retrieved knowledge to\\ngenerate the answer, which iterates only once.\\n3) Adaptive Retrieval. This baseline (Mallen et al.,\\n2023) adaptively augments the LLM with the re-\\ntrieval module, only when the entities appearing\\nin queries are less popular. To extract entities, we\\nuse the available entity-linking method (Li et al.,\\n2020), namely BLINK, for questions.\\n4) Self-RAG. This baseline (Asai et al., 2024)\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\\nTime per Query304050Performance (F1)  No Retrieval  Single-step Approach\\n  Adaptive Retrieval  Multi-step Approach  Adaptive-RAG (Ours)Performance vs Time with FLAN-T5-XXLFigure 5: QA performance (F1) and efficiency (Time/Query)\\nfor different retrieval-augmented generation approaches. We\\nuse the FLAN-T5-XXL (11B) as the base LLM.\\ntrains the LLM to adaptively perform retrieval and\\ngeneration, where the retrieval is conducted once it\\npredicts the special retrieval token above a certain\\nthreshold, and the answer generation follows.\\n5) Adaptive-RAG. This is our model that adap-\\ntively selects the retrieval-augmented generation\\nstrategy, smoothly oscillating between the non-\\nretrieval, single-step approach, and multi-step ap-\\nproaches4without architectural changes, based on\\nthe query complexity assessed by the classifier.\\n6) Multi-step Approach. This approach (Trivedi\\net al., 2023) is the multi-step retrieval-augmented\\nLLM, which iteratively accesses both the retriever\\nand LLM with interleaved Chain-of-Thought rea-\\nsoning (Wei et al., 2022b) repeatedly until it derives\\nthe solution or reaches the maximum step number.\\n7) Adaptive-RAG w/ Oracle This is an ideal sce-\\nnario of our Adaptive-RAG equipped with an or-\\nacle classifier that perfectly categorizes the query\\ncomplexity.\\nA.3 Implementation Details\\nFor computing resources, we use A100 GPUs\\nwith 80GB memory. In addition, due to the sig-\\nnificant costs associated with evaluating retrieval-\\naugmented generation models, we perform experi-\\nments with a single run. Finally, we implemented\\nmodels using PyTorch (Paszke et al., 2019) and\\nTransformers library (Wolf et al., 2020).\\nB Additional Experimental Results\\nPerformance vs Time We further provide a com-\\nparison of different retrieval-augmented genera-\\ntion approaches with FLAN-T5-XL and FLAN-T5-\\nXXL models in Figure 4 and Figure 5, respectively,\\nin the context of performance and efficiency trade-\\noffs. Similar to the observation made from the GPT-\\n3.5 model in Figure 1, our proposed Adaptive-RAG\\nis significantly more effective as well as efficient.\\n4For the multi-step approach, we use the state-of-the-art\\nquestion answering strategy from IRCoT (Trivedi et al., 2023).'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 14}, page_content='Table 7: Results on each of a collection of datasets with FLAN-T5-XXL (11B) as the LLM. We emphasize our results in bold.\\nSQuAD Natural Questions TriviaQA\\nData Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nSingle-stepSimpleNo Retrieval 7.00 14.40 8.40 0.00 0.08 18.80 25.50 20.40 0.00 0.08 32.80 39.20 35.40 0.00 0.08\\nSingle-step Approach 28.80 40.80 35.00 1.00 1.00 41.40 51.20 47.60 1.00 1.00 56.00 64.70 61.80 1.00 1.00\\nAdaptiveAdaptive Retrieval 15.60 25.60 20.00 0.50 0.54 31.00 39.70 35.00 0.50 0.54 44.80 52.20 48.60 0.50 0.54\\nSelf-RAG∗1.60 11.90 20.80 0.59 0.31 39.20 47.10 42.40 0.75 0.09 14.60 33.70 60.20 0.76 0.22\\nAdaptive-RAG (Ours) 27.80 39.80 34.00 1.17 1.50 41.20 51.00 47.40 1.00 1.00 52.00 60.30 57.20 1.03 1.33\\nComplex Multi-step Approach 24.60 36.90 30.20 2.13 3.83 39.60 49.60 46.40 2.16 3.94 52.60 61.10 59.40 2.17 4.03\\nOracle Adaptive-RAG w/ Oracle 32.80 46.90 38.20 0.85 0.94 51.20 61.00 57.00 0.71 0.91 63.40 71.30 68.20 0.51 0.60\\nMuSiQue HotpotQA 2WikiMultiHopQA\\nData Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nMulti-stepSimpleNo Retrieval 4.20 13.40 5.40 0.00 0.08 17.40 25.44 18.40 0.00 0.09 26.80 32.93 28.00 0.00 0.08\\nSingle-step Approach 16.80 25.70 19.20 1.00 1.00 37.60 49.27 39.60 1.00 1.00 46.60 54.13 48.20 1.00 1.00\\nAdaptiveAdaptive Retrieval 8.40 17.80 10.20 0.50 0.54 26.60 36.01 27.80 0.50 0.54 35.20 42.68 36.80 0.50 0.54\\nSelf-RAG∗1.20 8.20 11.80 0.68 0.27 5.60 17.86 30.60 0.76 0.26 3.00 19.14 39.00 0.90 0.25\\nAdaptive-RAG (Ours) 20.60 28.50 23.20 1.89 3.12 44.20 54.78 46.80 1.58 2.53 47.60 57.36 54.00 1.46 2.55\\nComplex Multi-step Approach 19.40 27.50 21.80 2.09 3.66 47.00 57.81 49.40 2.08 3.73 57.60 67.65 64.00 2.17 3.63\\nOracle Adaptive-RAG w/ Oracle 24.20 37.20 26.60 1.22 1.71 52.20 64.80 54.60 0.92 1.33 59.20 70.40 68.60 0.82 1.14\\nTable 8: Results on each of a collection of datasets with GPT-3.5 (Turbo) as the LLM. We emphasize our results in bold.\\nSQuAD Natural Questions TriviaQA\\nData Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nSingle-stepSimpleNo Retrieval 16.00 29.20 23.80 0.00 0.62 39.80 55.70 55.00 0.00 0.56 64.00 75.60 75.80 0.00 0.68\\nSingle-step Approach 18.00 33.80 29.20 1.00 1.00 32.40 46.80 54.80 1.00 1.00 55.20 66.50 65.80 1.00 1.00\\nAdaptiveAdaptive Retrieval 15.40 30.00 24.40 0.50 0.81 36.40 51.20 56.60 0.50 0.78 62.00 71.90 72.20 0.50 0.84\\nSelf-RAG∗1.60 11.90 20.80 0.59 1.91 39.20 47.10 42.40 0.75 0.52 14.60 33.70 60.20 0.76 1.59\\nAdaptive-RAG (Ours) 19.80 34.40 30.00 0.87 1.21 36.80 52.00 56.60 0.68 0.86 62.40 73.80 73.80 0.22 0.79\\nComplex Multi-step Approach 17.40 31.50 26.20 2.50 3.24 35.60 49.70 57.80 2.58 3.79 54.80 67.10 68.00 2.30 2.65\\nOracle Adaptive-RAG w/ Oracle 28.00 45.90 39.40 0.54 0.93 50.00 65.40 67.00 0.28 0.8 70.80 81.00 80.00 0.11 0.73\\nMuSiQue HotpotQA 2WikiMultiHopQA\\nData Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nMulti-stepSimpleNo Retrieval 20.40 31.30 24.40 0.00 0.81 37.40 51.04 43.20 0.00 0.74 37.00 48.50 43.40 0.00 0.90\\nSingle-step Approach 16.40 26.70 23.60 1.00 1.00 39.60 50.44 45.60 1.00 1.00 46.80 57.69 52.60 1.00 1.00\\nAdaptiveAdaptive Retrieval 18.80 30.30 24.80 0.50 0.90 38.60 50.70 43.20 0.50 0.87 44.20 55.11 50.60 0.50 0.95\\nSelf-RAG∗1.20 8.20 11.80 0.68 1.66 5.60 17.86 30.60 0.76 1.67 3.00 19.14 39.00 0.90 1.81\\nAdaptive-RAG (Ours) 21.80 32.60 29.60 1.90 2.29 40.40 52.56 47.00 0.93 1.48 46.60 60.09 56.80 1.59 2.23\\nComplex Multi-step Approach 23.00 32.50 31.60 3.41 3.61 45.80 58.36 52.20 2.73 3.18 52.20 66.08 62.40 3.36 3.35\\nOracle Adaptive-RAG w/ Oracle 29.60 44.70 35.60 0.90 1.45 55.60 69.90 62.80 0.54 1.08 52.20 69.90 66.60 0.65 1.21\\nPerformance per Dataset In addition to detail-\\ning the performance of each dataset with the FLAN-\\nT5-XL model, as shown in Table 2, we also present\\nthe results for each dataset with the FLAN-T5-\\nXXL and GPT-3.5 models in Table 2 and Table 8,\\nrespectively. The experimental results show that\\nour Adaptive-RAG consistently balances between\\nefficiency and accuracy. It is worth noting that\\nwhile the GPT-3.5 model performs effectively in\\naddressing straightforward queries even without\\ndocument retrieval, it benefits significantly from\\nour Adaptive-RAG in terms of effectiveness when\\nsolving complex multi-hop queries.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'RAG.pdf', 'page': 0}, page_content='Adaptive-RAG: Learning to Adapt Retrieval-Augmented\\nLarge Language Models through Question Complexity\\nSoyeong Jeong1Jinheon Baek2Sukmin Cho1Sung Ju Hwang1,2Jong C. Park1*\\nSchool of Computing1Graduate School of AI2\\nKorea Advanced Institute of Science and Technology1,2\\n{starsuzi,jinheon.baek,nelllpic,sjhwang82,jongpark}@kaist.ac.kr\\nAbstract\\nRetrieval-Augmented Large Language Models\\n(LLMs), which incorporate the non-parametric\\nknowledge from external knowledge bases into'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 0}, page_content='LLMs, have emerged as a promising approach\\nto enhancing response accuracy in several tasks,\\nsuch as Question-Answering (QA). However,\\neven though there are various approaches deal-\\ning with queries of different complexities, they\\neither handle simple queries with unnecessary\\ncomputational overhead or fail to adequately\\naddress complex multi-step queries; yet, not\\nall user requests fall into only one of the sim-\\nple or complex categories. In this work, we'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 0}, page_content='propose a novel adaptive QA framework that\\ncan dynamically select the most suitable strat-\\negy for (retrieval-augmented) LLMs from the\\nsimplest to the most sophisticated ones based\\non the query complexity. Also, this selec-\\ntion process is operationalized with a classi-\\nfier, which is a smaller LM trained to predict\\nthe complexity level of incoming queries with\\nautomatically collected labels, obtained from\\nactual predicted outcomes of models and in-\\nherent inductive biases in datasets. This ap-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 0}, page_content='proach offers a balanced strategy, seamlessly\\nadapting between the iterative and single-step\\nretrieval-augmented LLMs, as well as the no-\\nretrieval methods, in response to a range of\\nquery complexities. We validate our model\\non a set of open-domain QA datasets, cov-\\nering multiple query complexities, and show\\nthat ours enhances the overall efficiency and\\naccuracy of QA systems, compared to rele-\\nvant baselines including the adaptive retrieval\\napproaches. Code is available at: https://'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 0}, page_content='github.com/starsuzi/Adaptive-RAG .\\n1 Introduction\\nRecent Large Language Models (LLMs) (Brown\\net al., 2020; OpenAI, 2023; Touvron et al., 2023;\\nAnil et al., 2023) have shown overwhelming per-\\nformances across diverse tasks, including question-\\n*Corresponding author\\n0.5 1.0 1.5 2.0 2.5 3.0 3.5\\nTime per Query4748495051Performance (F1)  No Retrieval'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 0}, page_content='Single-step Approach  Adaptive Retrieval  Multi-step Approach   Adaptive-RAG (Ours)Performance vs Time with GPT-3.5Figure 1: QA performance (F1) and efficiency (Time/Query)\\nfor different retrieval-augmented generation approaches. We\\nuse the GPT-3.5-Turbo-Instruct as the base LLM.\\nanswering (QA) (Yang et al., 2018; Kwiatkowski\\net al., 2019). However, they still generate factu-\\nally incorrect answers since their knowledge solely\\nrelies on their parametric memory (Kasai et al.,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 0}, page_content='2022; Mallen et al., 2023). Meanwhile, memoriz-\\ning all the (ever-changing) world knowledge may\\nnot be possible. To address this problem, retrieval-\\naugmented LLMs (Borgeaud et al., 2022; Izacard\\net al., 2023; Shi et al., 2023), which incorporate\\nnon-parametric knowledge into LLMs with addi-\\ntional retrieval modules, have gained much increas-\\ning attention. Specifically, these models access\\na knowledge base, which serves as an extensive\\nrepository of information across various subjects'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 0}, page_content='and disciplines, to retrieve information relevant to\\nthe given input, and then incorporate the retrieved\\ninformation into LLMs, which enables them to stay\\naccurate and current with the world knowledge.\\nA particularly salient application of retrieval-\\naugmented LLMs is to handling QA tasks, whose\\ngoal is to provide correct answers in response to\\nuser queries, especially those of high complexity.\\nEarly work on retrieval-augmented LLMs focuses\\nprimarily on single-hop queries (Lazaridou et al.,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 0}, page_content='2022; Ram et al., 2023), whose answers are typ-\\nically found within a single document; therefore,\\nthis approach involves retrieving a relevant doc-\\nument based on the query and subsequently inte-\\ngrating this information into QA models to formu-\\nlate a response. However, unlike this single-hop\\nQA, some queries require connecting and aggregat-\\ning multiple documents, which are, furthermore,arXiv:2403.14403v2  [cs.CL]  28 Mar 2024'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='RetrievalComplex Query: \\nWhat currency is in \\nBilly Giles’ birthplace?Simple Query: \\nWhen is the birthday \\nof Michael F. Phelps?Documents\\nAnswer\\nDocuments\\nAnswer\\n(A) Single -Step Approach\\nInaccurate\\nRetrieval\\nRetrievalSimple Query: \\nWhen is the birthday \\nof Michael F. Phelps?Documents\\n(Intermediate) \\nAnswers\\n(B) Multi -Step Approach\\nInefficient\\nk times\\nComplex Query: \\nWhat currency is in \\nBilly Giles’ birthplace?Documents\\n(Intermediate) \\nAnswers\\nk timesStraightforward Query:'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='Paris is the capital of what?(C) Our Adaptive Approach\\nAnswer\\nSimple Query: \\nWhen is the birthday \\nof Michael F. Phelps?Documents\\nAnswer\\nComplex Query: \\nWhat currency is in \\nBilly Giles’ birthplace?Documents\\n(Intermediate) \\nAnswers\\nk times Classifier\\nFigure 2: A conceptual comparison of different retrieval-augmented LLM approaches to question answering. (A) In response to'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='a query, this single-step approach retrieves relevant documents and then generates an answer. However, it may not be sufficient\\nfor complex queries that require multi-step reasoning. (B) This multi-step approach iteratively retrieves documents and generates\\nintermediate answers, which is powerful yet largely inefficient for the simple query since it requires multiple accesses to both'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='LLMs and retrievers. (C) Our adaptive approach can select the most suitable strategy for retrieval-augmented LLMs, ranging\\nfrom iterative, to single, to even no retrieval approaches, based on the complexity of given queries determined by our classifier.\\noften not answerable through a single-step pro-\\ncess of retrieval-and-response. An example query\\nis ‘When did the people who captured Malakoff\\ncome to the region where Philipsburg is located?’,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='which requires four reasoning steps to solve. There-\\nfore, to effectively handle such complex queries,\\nrecent studies have concentrated largely on multi-\\nstep and multi-reasoning QA, which requires itera-\\ntive accesses to both LLMs and retrievers multiple\\ntimes (Press et al., 2023; Trivedi et al., 2023), at\\nthe cost of heavy computational overheads.\\nYet, we should rethink: In a real-world scenario,\\nare all the requests from users complex? Instead,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='users might often ask simple and straightforward\\nquestions, while only occasionally asking complex\\nones. Specifically, a query such as ‘Paris is the\\ncapital of what?’ is likely to be asked more fre-\\nquently, compared to the aforementioned multi-\\nstep query, and this simpler query might also be\\neasily answered by the LLMs themselves, without\\naccessing external knowledge. In other words, a\\nmulti-step QA approach could give rise to unnec-\\nessary computational overhead for simple queries,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='even though it would be vital for complex queries\\n(see Figure 2 (A)). On the other hand, handling\\ncomplex queries with single-step-retrieval or even\\nnon-retrieval strategies would be largely insuffi-\\ncient (Figure 2 (B)). This suggests the need for an\\nadaptive QA system, which can dynamically adjust\\nthe operational strategies of retrieval-augmented\\nLLMs based on the query complexity. While some\\nrecent approaches are capable of doing this based'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='on the frequency of entities in queries (Mallen et al.,\\n2023) or on the generated outputs from models\\nfor multi-step QA (Trivedi et al., 2023), they are\\nstill suboptimal: the former methods are overly\\nsimplistic, failing to consider multi-hop queries;\\nmeanwhile, the latter are excessively complex, ter-\\nminating answer solving steps after several rounds\\nof module access.In this work, considering diverse complexity lev-\\nels of real-world queries, we argue that previous'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='one-size-fits-all approaches might be inadequate to\\ncover all of them. Instead, we propose to select the\\nmost suitable strategy from a range of (retrieval-\\naugmented) LLMs, each of which is tailored to the\\nspecific complexity of the input query. Notably,\\na critical step in this process is pre-defining the\\nquery complexity, which is instrumental in deter-\\nmining the most fitting model to it. In this work,\\nwe operationalize this process with a novel classi-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='fier, which is a smaller model trained to predict the\\ncomplexity level of incoming queries (see Figure 2\\n(c)). Moreover, we automatically collect its training\\ndatasets without human labeling, by leveraging the\\npredicted outcomes (i.e., which models accurately\\nrespond to which queries) as well as by capitalizing\\non the inherent biases in existing datasets (i.e., sam-\\nples in the datasets are designed either for single-\\nstep or for multi-step QA scenarios). This proposed'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='method can offer a robust middle ground among the\\niterative LLM augmentation methods for complex\\nqueries, single-step methods for simpler queries,\\nand even no-retrieval-augmented methods for the\\nmost straightforward queries (answerable by LLMs\\nthemselves), thus significantly enhancing the over-\\nall efficiency and accuracy, as shown in Figure 1.\\nWe refer to our framework as Adaptive Retrieval-\\nAugmented Generation (Adaptive-RAG).\\nWe validate Adaptive-RAG using benchmark'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 1}, page_content='open-domain QA datasets, covering a wide range\\nof query complexity from single-hop (Rajpurkar\\net al., 2016; Joshi et al., 2017; Kwiatkowski et al.,\\n2019) to multi-hop (Yang et al., 2018; Ho et al.,\\n2020; Trivedi et al., 2022b) queries. The exper-\\nimental results show that ours significantly im-\\nproves the overall accuracy and efficiency, com-\\npared to the prior adaptive strategies, on multiple\\nLLMs, such as GPT-3.5 (Brown et al., 2020) and\\nFLAN-T5 series (Chung et al., 2022).'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 2}, page_content='Our contributions and findings are threefold:\\n•We point out the realistic scenario of queries of\\nvarying complexities, and find out that existing\\nretrieval-augmented generation approaches tend\\nto be overly simple or complex.\\n•We adapt retrieval-augmented LLMs to the query\\ncomplexity assessed by the classifier, which en-\\nables the utilization of the most suitable approach\\ntailored to each query.\\n•We show that our Adaptive-RAG is highly effec-\\ntive and efficient, balancing between the com-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 2}, page_content='plexity and the simplicity for diverse queries.\\n2 Related Work\\nOpen-domain QA Open-domain QA is the task\\nof accurately answering a query by sourcing for\\nquery-relevant documents, and then interpreting\\nthem to provide answers (Chen et al., 2017; Zhu\\net al., 2021), which, thus, generally involves two\\nmodules: a retriever (Karpukhin et al., 2020; Xiong\\net al., 2021) and a reader (Yang et al., 2019; Izac-\\nard and Grave, 2021; Jeong et al., 2023). Along\\nwith the emergence of LLMs with superior rea-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 2}, page_content='soning capabilities thanks to their billion-sized pa-\\nrameters (Wei et al., 2022a), a synergy between\\nLLMs and retrievers has led to significant advance-\\nments (Lazaridou et al., 2022; Ram et al., 2023).\\nSpecifically, this integration has been shown to\\nenhance Open-domain QA by mitigating the hallu-\\ncination problem from LLMs through strengthened\\nreasoning abilities of the reader, as well as utiliz-\\ning the retrieved, external documents (Cho et al.,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 2}, page_content='2023). Despite these advancements for single-hop\\nretrieval-augmented LLMs, however, the complex-\\nity of some queries needs a more complex strategy.\\nMulti-hop QA Multi-hop QA is an extension of\\nconventional Open-domain QA, which addition-\\nally requires the system to comprehensively gather\\nand contextualize information from multiple docu-\\nments (often iteratively), to answer more complex\\nqueries (Trivedi et al., 2022a; Yang et al., 2018). In\\nthe realm of multi-hop QA, the approach to itera-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 2}, page_content='tively access both LLMs and the retrieval module\\nis generally employed. Specifically, Khattab et al.\\n(2022), Press et al. (2023), Pereira et al. (2023)\\nand Khot et al. (2023) proposed to first decom-\\npose the multi-hop queries into simpler single-hop\\nqueries, repeatedly access the LLMs and retriever\\nto solve these sub-queries, and merge their solu-\\ntions to formulate a complete answer. In contrastto this decomposition-based approach, other re-\\ncent studies, such as Yao et al. (2023) and Trivedi'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 2}, page_content='et al. (2023), explored the interleaving of Chain-of-\\nThought reasoning (Wei et al., 2022b) — a method\\nwhere a logical sequence of thoughts is generated\\n— with document retrieval, repeatedly applying this\\nprocess until the reasoning chain generates the an-\\nswer. In addition, Jiang et al. (2023) introduced an\\napproach to repeatedly retrieving new documents\\nif the tokens within generated sentences have low\\nconfidence. However, the aforementioned methods'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 2}, page_content='overlooked the fact that, in real-world scenarios,\\nqueries are of a wide variety of complexities. There-\\nfore, it would be largely inefficient to iteratively\\naccess LLMs and retrievers for every query, which\\nmight be simple enough with a single retrieval step\\nor even only with an LLM itself.\\nAdaptive Retrieval To handle queries of varying\\ncomplexities, the adaptive retrieval strategy aims to\\ndynamically decide whether to retrieve documents\\nor not, based on each query’s complexity. In this'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 2}, page_content='vein, Mallen et al. (2023) proposed to decide the\\nquery’s complexity level based on the frequency of\\nits entities and suggested using the retrieval mod-\\nules only when the frequency falls below a cer-\\ntain threshold. However, this approach, focusing\\nsolely on the binary decision of whether to retrieve\\nor not, may not be sufficient for more complex\\nqueries that require multiple reasoning steps. Ad-\\nditionally, Qi et al. (2021) proposed an approach'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 2}, page_content='that performs a fixed set of operations (retrieving,\\nreading, and reranking) multiple times until the an-\\nswer is derived for the given query, which is built\\nupon traditional BERT-like LMs. However, unlike\\nour Adaptive-RAG which pre-determines the query\\ncomplexity and adapts the operational behavior of\\nany off-the-shelf LLMs accordingly, this approach\\napplies the same fixed operations to every query\\nregardless of its complexity but also necessitates'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 2}, page_content='additional specific training to LMs. Concurrent to\\nour work, Asai et al. (2024) suggested training a so-\\nphisticated model to dynamically retrieve, critique,\\nand generate the text. Nevertheless, we argue that\\nall the aforementioned adaptive retrieval methods\\nthat rely on a single model might be suboptimal in\\nhandling a variety of queries of a range of differ-\\nent complexities since they tend to be either overly\\nsimple or complex for all the input queries, which'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 2}, page_content='demands a new approach that can select the most\\nsuitable strategy of retrieval-augmented LLMs tai-\\nlored to the query complexity.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 3}, page_content='3 Method\\nIn this section, we describe our approach to adapt-\\ning retrieval-augmented LLMs, by pre-determining\\nthe query complexity and then selecting the most\\nfitting strategies for retrieval-augmented LLMs.\\n3.1 Preliminaries\\nWe begin with preliminaries, formally introducing\\ndifferent strategies of retrieval-augmented LLMs.\\nNon Retrieval for QA Let us first define an LLM\\nas a model LLM, which takes a sequence of tokens\\nx= [x1, x2, ..., x n]as an input and then generates'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 3}, page_content='a sequence of tokens y= [y1, y2, ..., y n]as an out-\\nput, which is formalized as follows: y=LLM(x).\\nThen, in our problem setup for QA, xandybe-\\ncome the input query ( q) from the user and the\\ngenerated answer ( ¯a) from the LLM, respectively:\\nq=xand¯a=y. Also, subsequently, the most\\nnaïve LLM-powered QA model can be represented\\nas follows: ¯a=LLM(q). Ideally, ¯ashould match\\nthe actual correct answer a. This non-retrieval-\\nbased QA method is highly efficient and could be'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 3}, page_content='a somewhat promising approach to handling easy\\nqueries, as the size of LLMs becomes extremely\\nlarge with its effect on storing a large amount of\\nknowledge. However, this approach is largely prob-\\nlematic on queries that require precise or concur-\\nrent knowledge of specific people, events, or any\\nsubjects beyond the LLMs’ internal knowledge.\\nSingle-step Approach for QA To address the\\naforementioned scenarios where LLMmay struggle\\nwith queries that are not answerable by LLMitself,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 3}, page_content='we can utilize the external knowledge d, which\\nincludes useful information for queries, retrieved\\nfrom the external knowledge source Dthat could\\nbe an encyclopedia (e.g., Wikipedia) consisting\\nof millions of documents. Specifically, to obtain\\nsuchdfromD, a specific retrieval model is nec-\\nessary, which returns documents based on their\\nrelevance with the given query. This process can\\nbe formulated as follows: d=Retriever (q;D),\\nwhere Retriever is the retrieval model, with'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 3}, page_content='d∈ D . Here, we can use any off-the-shelf re-\\ntriever (Robertson et al., 1994; Karpukhin et al.,\\n2020).\\nAfter the retrieval step is done, we now have a\\npair of query qand its relevant documents d. Then,\\nin order to augment LLMs with this retrieved exter-\\nnal knowledge, we can incorporate it into the input\\nof LLMs, represented as follows: ¯a=LLM(q,d).This process allows LLMs to gain access to exter-\\nnal information contained in d, which can provide'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 3}, page_content='the supplementary context that the internal knowl-\\nedge of LLMlacks, which can subsequently improve\\nthe accuracy and concurrency of LLMs for QA.\\nMulti-step Approach for QA Even though the\\naforementioned single-step approach offers signif-\\nicant improvements over non-retrieval for qthat\\nrequires external knowledge, it encounters notable\\nlimitations, particularly when dealing with com-\\nplex queries that necessitate synthesizing informa-\\ntion from multiple source documents and reasoning'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 3}, page_content='over them. This is where a multi-step approach and\\nreasoning for QA become essential.\\nIn this multi-step approach, LLMinteracts with\\nRetriever in several rounds, progressively refin-\\ning its understanding of q, until it formulates the fi-\\nnal answer from findings accumulated across these\\nmultiple steps. Specifically, the process begins\\nwith the initial query q, and at every retrieval step\\ni, new documents diare retrieved from Dand then\\nincorporated into the input of LLMs, as follows:'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 3}, page_content='¯ai=LLM(q,di,ci), where the additional context\\ncican be composed of previous documents and\\noutcomes (d1,d2, ...,di−1,¯a1,¯a2, ...,¯ai−1), and\\ndi=Retriever (q,ci;D)1. We would like to\\nnote that this iterative, multi-step process enables\\nLLMto construct a more comprehensive and exten-\\nsive foundation to solve queries effectively, specif-\\nically adept at complex multi-hop queries where\\nanswers depend on interconnected pieces of infor-\\nmation. However, it is important to recognize that'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 3}, page_content='this multi-step approach can be resource-intensive\\ndue to the repeated accesses to Retriever andLLM,\\nwhich entail substantial computational costs.\\n3.2 Adaptive-RAG: Adaptive\\nRetrieval-Augmented Generation\\nWe now introduce our adaptive retrieval-augmented\\nLLMs, which are built upon three different strate-\\ngies described in the previous section, and which\\nare designed to select the most suitable strategy\\naccording to the complexity of queries.\\nAdapting Retrieval-Augmented LLMs Note'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 3}, page_content='that in real-world scenarios, not all qfrom users\\nhave the same level of complexity, necessitating\\n1It is worth noting that implementations of the LLM and\\nretriever vary across different multi-step retrieval-augmented\\nLLM approaches (Trivedi et al., 2023; Press et al., 2023; Yao\\net al., 2023); therefore, the context cimay incorporate none,\\nsome, or all of the previous documents and answers.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 4}, page_content='tailored strategies for handling each query. In other\\nwords, employing the most basic, non-retrieval-\\nbased approach LLM(q)to respond to the complex\\nquery qwould be also ineffective (Figure 2, A);\\nconversely, using a more elaborate multi-step ap-\\nproach LLM(q,d,c)for simple qwould be ineffi-\\ncient (Figure 2, B). Therefore, our adaptive frame-\\nwork is designed to dynamically adjust the query-\\nhandling strategy of retrieval-augmented LLMs,\\nwhich is achieved by determining the complexity of'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 4}, page_content='each query before attempting a solution. Notably,\\nthis framework can offer a robust middle ground\\nwith a range of solutions, from the simplest ap-\\nproach for the most straightforward queries, to the\\none-step approach for moderate queries, and up to\\nthe most comprehensive and rigorous approach for\\ncomplex queries. In addition, since the operations\\nofLLMandRetriever remain consistent regard-\\nless of inputs to them, our method can seeming-\\nlessly go back and forth across queries of different'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 4}, page_content='complexities, without changing the internal model\\narchitecture or parameters during adaption.\\nQuery Complexity Assessment To operational-\\nize our adaptive retrieval-augmented LLM frame-\\nwork, we should determine the query complexity,\\nand to achieve this, we propose to model a com-\\nplexity classifier, whose goal is to return the appro-\\npriate complexity level of the given query. Specif-\\nically, given the query q, our classifier can be for-\\nmulated as follows: o=Classifier (q), where'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 4}, page_content='Classifier is a smaller Language Model that is\\ntrained to classify one of three different complexity\\nlevels and ois its corresponding class label. In our\\nclassifier design, there are three class labels: ‘A’,\\n‘B’, and ‘C’, where ‘A’ indicates that qis straight-\\nforward and answerable by LLM(q)itself, ‘B’ in-\\ndicates that qhas the moderate complexity where\\nat least a single-step approach LLM(q,d)is needed,\\nand ‘C’ indicates that qis complex, requiring the\\nmost extensive solution LLM(q,d,c)2.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 4}, page_content='Training Strategy The remaining step is to train\\nthe smaller Language Model for Classifier , to\\naccurately predict its complexity oin response to\\nthe given query q. Yet, there is no annotated dataset\\navailable for query-complexity pairs. Hence, we\\npropose to automatically construct the training\\ndataset with two particular strategies.\\nTo be specific, we first aim at labeling the query\\n2We consider three levels of query complexity, and leave'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 4}, page_content='the exploration of more fine-grained complexities as future\\nwork.complexity based on the results from three different\\nretrieval-augmented LLM strategies, in order to\\ndetermine the label by its needs. For example, if\\nthe simplest non-retrieval-based approach correctly\\ngenerates the answer, the label for its corresponding\\nquery is assigned ‘A’. Also, to break the tie between\\ndifferent models in providing the label to the query,\\nwe provide a higher priority to a simpler model.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 4}, page_content='In other words, if both single-step and multi-step\\napproaches produce the same correct answer while\\nthe non-retrieval-based approach fails, we assign\\nlabel ‘B’ to its corresponding query.\\nHowever, this labeling strategy has a limita-\\ntion in that not all the queries are assigned labels,\\nsince the three retrieval-augmented approaches\\nmay all fail to generate the correct answer. On\\nthe other hand, the benchmark datasets may al-\\nready have meaningful inductive biases about the'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 4}, page_content='most appropriate retrieval-augmented LLM strate-\\ngies for their queries, considering the ways they\\nare created (e.g., QA datasets that require sequen-\\ntial reasoning usually necessitate a multi-step ap-\\nproach; while queries of those with labeled sin-\\ngle documents can be ideally answerable with the\\nsingle-step approach). Therefore, for those queries\\nthat remain unlabeled after the first labeling step,\\nwe assign ‘B’ to queries in single-hop datasets'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 4}, page_content='and ‘C’ to queries in multi-hop datasets. Finally,\\nwe train Classifier with these automatically-\\ncollected query-complexity pairs3, by using a cross-\\nentropy loss. Then, at inference, we can deter-\\nmine the complexity of the query, which is one of\\n{‘A’, ‘B’, ‘C’}, by forwarding it to Classifier :\\no=Classifier (q).\\n4 Experimental Setups\\nIn this section, we explain datasets, models, met-\\nrics, and implementation details. We provide addi-\\ntional details in Appendix A.\\n4.1 Datasets'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 4}, page_content='4.1 Datasets\\nIn order to simulate a realistic scenario, where dif-\\nferent queries have varying complexities, we use\\nboth the single-hop and multi-hop QA datasets si-\\nmultaneously, in the unified experimental setting.\\nSingle-hop QA For simpler queries, we use three\\nbenchmark single-hop QA datasets, which consist\\n3As we automatically assign classifier labels, there might\\nbe errors in labeling and might be more advanced strategies to\\nautomatically assign labels, which we leave as future work.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 5}, page_content='Table 1: Averaged results on a collection of benchmark datasets for open-domain question answering including the single-hop\\nand multi-hop queries, with different LLMs. Self-RAG∗is trained with a different base LLM, namely LLaMA2 (Touvron et al.,\\n2023); therefore, we compare the results of FLAN-T5-XL (3B) with the results from Self-RAG with LLaMA2 (7B) and the\\nresults of others with the results from Self-RAG with LLaMA2 (13B). We emphasize our results in bold, for easy comparisons.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 5}, page_content='FLAN-T5-XL (3B) FLAN-T5-XXL (11B) GPT-3.5 (Turbo)\\nTypes Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nSimpleNo Retrieval 14.87 21.12 15.97 0.00 0.11 17.83 25.14 19.33 0.00 0.08 35.77 48.56 44.27 0.00 0.71\\nSingle-step Approach 34.83 44.31 38.87 1.00 1.00 37.87 47.63 41.90 1.00 1.00 34.73 46.99 45.27 1.00 1.00\\nAdaptiveAdaptive Retrieval 23.87 32.24 26.73 0.50 0.56 26.93 35.67 29.73 0.50 0.54 35.90 48.20 45.30 0.50 0.86'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 5}, page_content='Self-RAG∗9.90 20.79 31.57 0.72 0.43 10.87 22.98 34.13 0.74 0.23 10.87 22.98 34.13 0.74 1.50\\nAdaptive-RAG (Ours) 37.17 46.94 42.10 2.17 3.60 38.90 48.62 43.77 1.35 2.00 37.97 50.91 48.97 1.03 1.46\\nComplex Multi-step Approach 39.00 48.85 43.70 4.69 8.81 40.13 50.09 45.20 2.13 3.80 38.13 50.87 49.70 2.81 3.33\\nOracle Adaptive-RAG w/ Oracle 45.00 56.28 49.90 1.28 2.11 47.17 58.60 52.20 0.84 1.10 47.70 62.80 58.57 0.50 1.03\\nof queries and their associated documents contain-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 5}, page_content='ing answers, namely 1) SQuAD v1.1 (Rajpurkar\\net al., 2016), 2) Natural Questions (Kwiatkowski\\net al., 2019), and 3) TriviaQA (Joshi et al., 2017).\\nMulti-hop QA To consider more complex query\\nscenarios, we use three benchmark multi-hop QA\\ndatasets, which require sequential reasoning over\\nmultiple documents, namely 1) MuSiQue (Trivedi\\net al., 2022a), 2) HotpotQA (Yang et al., 2018),\\nand3) 2WikiMultiHopQA (Ho et al., 2020).\\n4.2 Models\\nWe compare our Adaptive-RAG against relevant'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 5}, page_content='models, including three retrieval-augmented LLM\\nstrategies (in Section 3.1) and the adaptive re-\\ntrieval approaches (Mallen et al., 2023; Asai et al.,\\n2024), which can be grouped into one of three cat-\\negories: Simple, Adaptive, and Complex. Specif-\\nically, Simple approaches include the 1) No Re-\\ntrieval and2) Single-step Approach -based meth-\\nods. Adaptive approaches include the 3) Adaptive\\nRetrieval (Mallen et al., 2023), 4) Self-RAG (Asai\\net al., 2024), and our 5) Adaptive-RAG , which'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 5}, page_content='can adaptively perform retrieval based on the\\nquestion complexity. For the 6) Multi-step Ap-\\nproach , we use the most sophisticated state-of-\\nthe-art method (Trivedi et al., 2023), iteratively\\naccessing both the retriever and LLM with Chain-\\nof-Thought reasoning (Wei et al., 2022b), for every\\nquery. Note that models across different categories\\nare not directly comparable. Yet, in the ideal set-\\nting, Adaptive approaches should be more effective\\nthan those in the Simple category while simultane-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 5}, page_content='ously being more efficient than the Complex one.\\nTherefore, we also report the performance in an\\nideal scenario, 7) Adaptive-RAG w/ Oracle , using\\nthe oracle classifier with our Adaptive-RAG.\\n4.3 Evaluation Metrics\\nWhen it comes to evaluating adaptive models, it\\nis essential to simultaneously consider both thetask performance and efficiency along with their\\ntrade-offs. Thus, we report the results with five\\nmetrics, where three of them measure the effective-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 5}, page_content='ness and the other two measure the efficiency. In\\nparticular, for effectiveness, we use F1, EM, and\\nAccuracy (Acc), following the standard evaluation\\nprotocol (Mallen et al., 2023; Baek et al., 2023;\\nAsai et al., 2024), where F1 measures the number\\nof overlapping words between the predicted an-\\nswer and the ground truth, EM measures whether\\nthey are the same, and Acc measures whether the\\npredicted answer contains the ground-truth answer.\\nFor efficiency, we measure the number of retrieval-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 5}, page_content='and-generate steps and the average time for answer-\\ning each query relative to the one-step approach.\\n4.4 Implementation Details\\nFor a fair comparison and following Mallen et al.\\n(2023) and Trivedi et al. (2023), we use the same re-\\ntriever, a term-based sparse retrieval model known\\nas BM25 (Robertson et al., 1994), across all differ-\\nent models. For the external document corpus, we\\nuse different sources depending on the dataset type:\\nthe Wikipedia corpus preprocessed by Karpukhin'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 5}, page_content='et al. (2020) for single-hop datasets, and the pre-\\nprocessed corpus by Trivedi et al. (2023) for multi-\\nhop datasets. Regarding the LLMs that are used\\nto generate answers, we use the FLAN-T5 series\\nmodels (Chung et al., 2022) of XL with 3B pa-\\nrameters and XXL with 11B parameters, and the\\nGPT-3.5 model (gpt-3.5-turbo-instruct). For the\\nretrieval-augmented LLM design, we follow the\\nimplementation details from Trivedi et al. (2023),\\nwhich include input prompts, instructions, and the'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 5}, page_content='number of test samples for evaluation (e.g., 500\\nsamples per dataset). In our Adaptive-RAG, for the\\nquery-complexity classifier, we use and train the\\nT5-Large model (Raffel et al., 2020). Specifically,\\nthe classifier is trained using the epoch that shows\\nthe best performance until 100 training iterations\\nfrom the validation set, with the learning rate of 3e-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='Table 2: Results on each of a collection of datasets with FLAN-T5-XL (3B) as the LLM. We emphasize our results in bold.\\nSQuAD Natural Questions TriviaQA\\nData Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nSingle-stepSimpleNo Retrieval 3.60 10.50 5.00 0.00 0.11 14.20 19.00 15.60 0.00 0.13 25.00 31.80 27.00 0.00 0.13\\nSingle-step Approach 27.80 39.30 34.00 1.00 1.00 37.80 47.30 44.60 1.00 1.00 53.60 62.40 60.20 1.00 1.00'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='AdaptiveAdaptive Retrieval 13.40 23.10 17.60 0.50 0.55 28.20 36.00 33.00 0.50 0.56 38.40 46.90 42.60 0.50 0.56\\nSelf-RAG∗2.20 11.20 18.40 0.63 0.50 31.40 39.00 33.60 0.63 0.17 12.80 29.30 57.00 0.68 0.45\\nAdaptive-RAG (Ours) 26.80 38.30 33.00 1.37 2.02 37.80 47.30 44.60 1.00 1.00 52.20 60.70 58.20 1.23 1.54\\nComplex Multi-step Approach 24.40 35.60 29.60 4.52 9.03 38.60 47.80 44.20 5.04 10.18 53.80 62.40 60.20 5.28 9.22'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='Oracle Adaptive-RAG w/ Oracle 32.00 45.60 38.20 1.24 1.60 47.40 57.10 53.60 1.10 1.55 61.60 70.20 66.40 0.79 1.10\\nMuSiQue HotpotQA 2WikiMultiHopQA\\nData Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nMulti-stepSimpleNo Retrieval 2.40 10.70 3.20 0.00 0.11 16.60 22.71 17.20 0.00 0.11 27.40 32.04 27.80 0.00 0.10\\nSingle-step Approach 13.80 22.80 15.20 1.00 1.00 34.40 46.15 36.40 1.00 1.00 41.60 47.90 42.80 1.00 1.00'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='AdaptiveAdaptive Retrieval 6.40 15.80 8.00 0.50 0.55 23.60 32.22 25.00 0.50 0.55 33.20 39.44 34.20 0.50 0.55\\nSelf-RAG∗1.60 8.10 12.00 0.73 0.51 6.80 17.53 29.60 0.73 0.45 4.60 19.59 38.80 0.93 0.49\\nAdaptive-RAG (Ours) 23.60 31.80 26.00 3.22 6.61 42.00 53.82 44.40 3.55 5.99 40.60 49.75 46.40 2.63 4.68\\nComplex Multi-step Approach 23.00 31.90 25.80 3.60 7.58 44.60 56.54 47.00 5.53 9.38 49.60 58.85 55.40 4.17 7.37'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='Oracle Adaptive-RAG w/ Oracle 24.80 38.50 27.00 1.98 3.99 51.20 64.00 54.80 1.59 2.77 53.00 62.30 59.40 1.01 1.69\\nF1102030405060Adaptive Retrieval\\nSelf-RAG\\nAdaptive-RAG (Ours)\\nClassifier Acc.3540455055FLAN-T5-XL\\nF1102030405060Adaptive Retrieval\\nSelf-RAG\\nAdaptive-RAG (Ours)\\nClassifier Acc.3540455055FLAN-T5-XXL\\nNo One MultiNo One Multi0.31 0.47 0.22\\n0.1 0.66 0.23\\n0.03 0.31 0.65\\n0.20.40.6Confusion Matrix'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='0.20.40.6Confusion Matrix\\nFigure 3: Performance on QA and query-complexity assessment of different adaptive approaches for retrieval-augmented LLMs\\nwith FLAN-T5 XL (Left) and XXL (Center). For labeling the complexity of queries, we use the silver data annotated from the\\nprediction outcomes of models (described in Section 3.2). We also provide the confusion matrix across three labels (Right).\\n5 and the AdamW (Loshchilov and Hutter, 2019)\\nas an optimizer. Regarding its training data, we'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='sample and annotate 400 queries from 6 datasets\\nbased on its inductive bias (single-hop for one-step\\napproach and multi-hop for multi-step). In addition,\\nwe use predicted outcomes of three different strate-\\ngies over 400 queries sampled from each dataset.\\nNote that those queries used for classifier training\\ndo not overlap with the testing queries for QA.\\n5 Experimental Results and Analyses\\nIn this section, we show the overall experimental\\nresults and offer in-depth analyses of our method.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='Main Results First of all, Table 1 shows our main\\nresults averaged over all considered datasets, which\\ncorroborate our hypothesis that simple retrieval-\\naugmented strategies are less effective than the\\ncomplex strategy, while the complex one is sig-\\nnificantly more expensive than the simple ones. In\\naddition, we report the more granular results with\\nFLAN-T5-XL on each of the single-hop and multi-\\nhop datasets in Table 2 (and more with different\\nLLMs in Table 7 and Table 8 of Appendix), which'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='are consistent with the results observed in Table 1.\\nHowever, in a real-world scenario, not all users\\nask queries with the same level of complexity,\\nwhich emphasizes the importance of the need for\\nadaptive strategies. Note that among the adaptive\\nstrategies, our Adaptive-RAG shows remarkableeffectiveness over the competitors (Table 1). This\\nindicates that merely focusing on the decision of\\nwhether to retrieve or not is suboptimal. Also, as\\nshown in Table 2, such simple adaptive strategies'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='are particularly inadequate for handling complex\\nqueries in multi-hop datasets, which require ag-\\ngregated information and reasoning over multiple\\ndocuments. Meanwhile, our approach can consider\\na more fine-grained query handling strategy by fur-\\nther incorporating an iterative module for complex\\nqueries. Furthermore, in a realistic setting, we\\nshould take into account not only effectiveness but\\nalso efficiency. As shown in Table 1, compared to'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='the complex multi-step strategy, our proposed adap-\\ntive strategy is significantly more efficient across\\nall model sizes. This is meaningful in this era of\\nLLMs, where the cost of accessing them is a critical\\nfactor for practical applications and scalability. Fi-\\nnally, to see the upper bound of our Adaptive-RAG,\\nwe report its performances with the oracle classifier\\nwhere the classification performance is perfect. As\\nshown in Table 1 and Table 2, we observe that it'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 6}, page_content='achieves the best performance while being much\\nmore efficient than our Adaptive-RAG without the\\noracle classifier. These results support the valid-\\nity and significance of our proposal for adapting\\nretrieval-augmented LLM strategies based on query\\ncomplexity, and further suggest the direction to de-\\nvelop more improved classifiers to achieve optimal\\nperformance.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 7}, page_content='Table 3: The exact elapsed time per query and the percentage\\nof the predicted labels from the classifier over all samples.\\nLabels Time/Query (Sec.) Percentage (%)\\nNo (A) 0.35 8.60\\nOne (B) 3.08 53.33\\nMulti (C) 27.18 38.07\\nClassifier Performance To understand how the\\nproposed classifier works, we analyze its perfor-\\nmance across different complexity labels. As Fig-\\nure 3 (Left and Center) shows, the classification\\naccuracy of our Adaptive-RAG is better than those'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 7}, page_content='of the other adaptive retrieval baselines, which\\nleads to overall QA performance improvements. In\\nother words, this result indicates that our Adaptive-\\nRAG is capable of more accurately classifying the\\ncomplexity levels with various granularities, which\\ninclude not performing retrieval, performing re-\\ntrieval only once, and performing retrieval multiple\\ntimes. In addition to the true positive performance\\nof our classifier averaged over all those three la-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 7}, page_content='bels in Figure 3 (Left and Center), we further re-\\nport its confusion matrix in Figure 3 (Right). We\\nnote that the confusion matrix reveals some notable\\ntrends: ‘C (Multi)’ is sometimes misclassified as\\n‘B (One)’ (about 31%) and ‘B (One)’ as ‘C (Multi)’\\n(about 23%); ‘A (No)’ is misclassified often as\\n‘B (One)’ (about 47%) and less frequently as ‘C\\n(Multi)’ (about 22%). While the overall results in\\nFigure 3 show that our classifier effectively cate-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 7}, page_content='gorizes the three labels, further refining it based\\non such misclassification would be a meaningful\\ndirection for future work.\\nAnalyses on Efficiency for Classifier While Ta-\\nble 1 shows the relative elapsed time for each of the\\nthree different RAG strategies, we further provide\\nthe exact elapsed time per query for our Adaptive-\\nRAG and the distribution for predicted labels from\\nour query-complexity classifier in Table 3. Similar\\nto the results of the elapsed time in Table 1 (relative'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 7}, page_content='time), Table 3 (exact time) shows that efficiency\\ncan be substantially improved by identifying sim-\\nple or straightforward queries.\\nAnalyses on Training Data for Classifier We\\nhave shown that the classifier plays an important\\nrole in adaptive retrieval. Here, we further analyze\\nthe different strategies for training the classifier by\\nablating our full training strategy, which includes\\ntwo approaches: generating silver data from pre-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 7}, page_content='dicted outcomes of models and utilizing inductiveTable 4: Results on QA and complexity classification with\\nvarying the data annotation strategies for training the classifier.\\nQA Classifier (Accuracy)\\nTraining Strategies F1 Step All No One Multi\\nAdaptive-RAG (Ours) 46.94 1084 54.52 30.52 66.28 65.45\\nw/o Binary 43.43 640 60.30 62.19 65.70 39.55\\nw/o Silver 48.79 1464 40.00 0.00 53.98 75.91\\nbias in datasets (see Section 3.2). As Table 4 shows,\\ncompared to the training strategy relying solely on'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 7}, page_content='the data derived from inductive bias, ours is sig-\\nnificantly more efficient. This efficiency is partly\\nbecause ours also takes into account the case that\\ndoes not consider any documents at all, as also\\nimplied by the classification accuracy; meanwhile,\\nqueries in the existing datasets do not capture the\\ninformation on whether the retrieval is required or\\nnot. On the other hand, in the case of only using the\\nsilver data annotated from the correct predictions,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 7}, page_content='while its overall classification accuracy is high, the\\noverall QA performance implies that relying on\\nthe silver data may not be optimal. This may be\\nbecause this silver data does not cover complex-\\nity labels over incorrectly predicted queries, which\\nleads to lower generalization effect on queries rel-\\nevant to them. Meanwhile, by also incorporating\\ncomplexity labels from dataset bias (single-hop vs\\nmulti-hop), the classifier becomes more accurate in'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 7}, page_content='predicting multi-hop queries, leading to the better\\nperformance. It is worth noting that our automatic\\nlabeling strategies are two particular instantiations\\nfor training the classifier, and that there could be\\nother instantiations, which we leave as future work.\\nAnalyses on Classifier Size To investigate the\\nsensitivity of our classifier according to its varying\\nsizes, we conducted further experiments. As shown\\nin Table 6, we observe no significant performance'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 7}, page_content='differences among classifiers of various sizes, even\\nwith reduced complexity and fewer parameters in\\nsmaller classifiers. This indicates that our proposed\\nclassifier can contribute to resource-efficient set-\\ntings in real-use cases with smaller sizes without\\ncompromising the performance.\\nCase Study We conduct a case study to qualita-\\ntively compare our Adaptive-RAG against Adap-\\ntive Retrieval. Table 5 shows the classified com-\\nplexity and the query handling patterns for both'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 7}, page_content='simple and complex questions. First, for the simple\\nsingle-hop question, our Adaptive-RAG identifies\\nthat it is answerable by only using the LLM’s para-\\nmetric knowledge about ‘Google’. By contrast,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 8}, page_content='Table 5: Case study with GPT-3.5 (Turbo), where we represent the factual error in red and the accurate information in blue.\\nDataset Question Adaptive Retrieval Adaptive-RAG (Ours)\\nNQ\\n(Single-hop)Which famous corpo-\\nrate logo changed to a\\nflat colour/color sans\\nserif font in its first ma-\\njor change since 1999?Query Type: B (Single-step Approach)\\nReasoning: The famous corporate logo that\\nchanged to a flat color sans-serif font in its first\\nmajor change since 1999 is Microsoft. Microsoft'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 8}, page_content='changed its logo in 2012, replacing the italicized,\\nmulti-colored font with a simple, flat, grey font. So\\nthe answer is: Microsoft.Query Type: A (Non Retrieval)\\nReasoning: The famous corporate logo that\\nchanged to a flat color sans serif font in its first ma-\\njor change since 1999 is Google. Google changed\\nits logo in 2015. So the answer is: Google.\\nMuSiQue\\n(Multi-hop)Who is the child of the\\nItalian navigator who\\nexplored the eastern\\ncoast of the continent\\nCésar Gaytan was born'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 8}, page_content='César Gaytan was born\\nin for the English?Query Type: A (Non Retrieval)\\nReasoning: César Gaytan was born in the conti-\\nnent of North America. The Italian navigator who\\nexplored the eastern coast of North America for the\\nEnglish was Giovanni Caboto, also known as John\\nCabot. So the answer is: Giovanni Caboto/John\\nCabot.Query Type: C (Multi-step Approach)\\nReasoning: Césarr Gaytan was born in Guadalajara,\\nJalisco, Mexico. The Italian navigator who explored'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 8}, page_content='the eastern coast of the continent for the English is\\nJohn Cabot. John Cabot’s son is Sebastian Cabot.\\nSo the answer is: Sebastian Cabot.\\nTable 6: Results with varying model sizes for classifiers.\\nQA Classifier (Accuracy)\\nSizes F1 Step All No One Multi\\nSmall (60M) 45.83 964 53.48 26.65 70.62 53.18\\nBase (223M) 45.97 983 53.41 26.42 69.46 56.82\\nLarge (770M) 46.94 1084 54.52 30.52 66.28 65.45\\nAdaptive Retrieval fetches additional documents,\\nleading to longer processing times and occasion-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 8}, page_content='ally producing incorrect responses due to the inclu-\\nsion of partially irrelevant information about ‘Mi-\\ncrosoft’. Meanwhile, faced with a complex ques-\\ntion, Adaptive-RAG seeks out relevant information,\\nincluding details like ‘a son of John Cabot’, which\\nmay not have been stored in LLMs, while Adaptive\\nRetrieval fails to request such information from\\nexternal sources, resulting in inaccurate answers.\\n6 Conclusion\\nIn this work, we proposed the Adaptive Retrieval-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 8}, page_content='Augmented Generation framework, referred to\\nas Adaptive-RAG, to handle queries of various\\ncomplexities. Specifically, Adaptive-RAG is de-\\nsigned to dynamically adjust its query handling\\nstrategies in the unified retrieval-augmented LLM\\nbased on the complexity of queries that they en-\\ncounter, which spans across a spectrum of the non-\\nretrieval-based approach for the most straightfor-\\nward queries, to the single-step approach for the\\nqueries of moderate complexity, and finally to the'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 8}, page_content='multi-step approach for the complex queries. The\\ncore step of our Adaptive-RAG lies in determin-\\ning the complexity of the given query, which is\\ninstrumental in selecting the most suitable strat-\\negy for its answer. To operationalize this process,\\nwe trained a smaller Language Model with query-\\ncomplexity pairs, which are automatically anno-\\ntated from the predicted outcomes and the inductive\\nbiases in datasets. We validated our Adaptive-RAGon a collection of open-domain QA datasets, cover-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 8}, page_content='ing the multiple query complexities including both\\nthe single- and multi-hop questions. The results\\ndemonstrate that our Adaptive-RAG enhances the\\noverall accuracy and efficiency of QA systems, al-\\nlocating more resources to handle complex queries\\nwhile efficiently handling simpler queries, com-\\npared to the existing one-size-fits-all approaches\\nthat tend to be either minimalist or maximalist over\\nvarying query complexities.\\nLimitations\\nWhile our Adaptive-RAG shows clear advantages'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 8}, page_content='in effectiveness and efficiency by determining the\\nquery complexity and then leveraging the most\\nsuitable approach for tackling it, it is important\\nto recognize that there still exist potential avenues\\nfor improving the classifier from the perspectives\\nof its training datasets and architecture. Specifi-\\ncally, as there are no available datasets for training\\nthe query-complexity classifier, we automatically\\ncreate new data based on the model prediction out-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 8}, page_content='comes and the inductive dataset biases. However,\\nour labeling process is one specific instantiation\\nof labeling the query complexity, and it may have\\nthe potential to label queries incorrectly despite its\\neffectiveness. Therefore, future work may create\\nnew datasets that are annotated with a diverse range\\nof query complexities, in addition to the labels of\\nquestion-answer pairs. Also, as the performance\\ngap between the ideal classifier in Table 1 and the'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 8}, page_content='current classifier in Figure 3 indicates, there is still\\nroom to improve the effectiveness of the classifier.\\nIn other words, our classifier design based on the\\nsmaller LM is the initial, simplest instantiation for\\nclassifying the query complexity, and based upon\\nit, future work may improve the classifier archi-\\ntecture and its performance, which will positively\\ncontribute to the overall QA performance.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 9}, page_content='Ethics Statement\\nThe experimental results on Adaptive-RAG vali-\\ndate its applicability in realistic scenarios, where a\\nwide range of diverse user queries exist. Nonethe-\\nless, given the potential diversity of real-world user\\ninputs, it is crucial to also consider scenarios where\\nthese inputs might be offensive or harmful. We\\nshould be aware that such inputs could lead to the\\nretrieval of offensive documents and the genera-\\ntion of inappropriate responses by the retrieval-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 9}, page_content='augmented LLMs. To address this challenge, de-\\nveloping methods to detect and manage offensive\\nor inappropriate content in both user inputs and re-\\ntrieved documents within the retrieval-augmented\\nframework is essential. We believe that this is a\\ncritical area for future work.\\nAcknowledgements\\nThis work was supported by Institute for Informa-\\ntion and communications Technology Promotion\\n(IITP) grant funded by the Korea government (No.\\n2018-0-00582, Prediction and augmentation of the'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 9}, page_content='credibility distribution via linguistic analysis and\\nautomated evidence document collection), Basic\\nScience Research Program through the National\\nResearch Foundation of Korea (NRF) funded by the\\nMinistry of Education (RS-2023-00275747), and\\nthe Artificial intelligence industrial convergence\\ncluster development project funded by the Ministry\\nof Science and ICT (MSIT, Korea) & Gwangju\\nMetropolitan City.\\nReferences\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 9}, page_content='son, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\\nChen, Eric Chu, Jonathan H. Clark, Laurent El\\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\\nYuanzhong Xu, Yujing Zhang, Gustavo Hernández\\nÁbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\\nJan A. Botha, James Bradbury, Siddhartha Brahma,\\nKevin Brooks, Michele Catasta, Yong Cheng, Colin'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 9}, page_content='Cherry, Christopher A. Choquette-Choo, Aakanksha\\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\\nNan Du, Ethan Dyer, Vladimir Feinberg, Fangxi-\\naoyu Feng, Vlad Fienber, Markus Freitag, Xavier\\nGarcia, Sebastian Gehrmann, Lucas Gonzalez, and\\net al. 2023. Palm 2 technical report. arXiv preprint\\narXiv:2305.10403 .\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, andHannaneh Hajishirzi. 2024. Self-RAG: Learning to'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 9}, page_content='retrieve, generate, and critique through self-reflection.\\nInThe Twelfth International Conference on Learning\\nRepresentations .\\nJinheon Baek, Soyeong Jeong, Minki Kang, Jong Park,\\nand Sung Ju Hwang. 2023. Knowledge-augmented\\nlanguage model verification. In Proceedings of the\\n2023 Conference on Empirical Methods in Natural\\nLanguage Processing, EMNLP 2023, Singapore, De-\\ncember 6-10, 2023 , pages 1720–1736. Association\\nfor Computational Linguistics.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 9}, page_content='Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\\n2022. Improving language models by retrieving from'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 9}, page_content='trillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA , volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240.\\nPMLR.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 9}, page_content='Clemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In Ad-\\nvances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December 6-12,\\n2020, virtual .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 9}, page_content='Bordes. 2017. Reading wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, ACL 2017, Vancouver, Canada, July 30 -\\nAugust 4, Volume 1: Long Papers , pages 1870–1879.\\nAssociation for Computational Linguistics.\\nSukmin Cho, Jeongyeon Seo, Soyeong Jeong, and\\nJong C. Park. 2023. Improving zero-shot reader by\\nreducing distractions from irrelevant documents in\\nopen-domain question answering. In Findings of the'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 9}, page_content='Association for Computational Linguistics: EMNLP\\n2023, Singapore, December 6-10, 2023 , pages 3145–\\n3157. Association for Computational Linguistics.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 10}, page_content='Narang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\\n2022. Scaling instruction-finetuned language models.\\narXiv preprint arXiv:2210.11416 .\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\\nand Akiko Aizawa. 2020. Constructing A multi-hop\\nQA dataset for comprehensive evaluation of reason-\\ning steps. In Proceedings of the 28th International'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 10}, page_content='Conference on Computational Linguistics, COLING\\n2020, Barcelona, Spain (Online), December 8-13,\\n2020 , pages 6609–6625. International Committee on\\nComputational Linguistics.\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open do-\\nmain question answering. In Proceedings of the 16th\\nConference of the European Chapter of the Associ-\\nation for Computational Linguistics: Main Volume,\\nEACL 2021, Online, April 19 - 23, 2021 , pages 874–'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 10}, page_content='880. Association for Computational Linguistics.\\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\\nEdouard Grave. 2023. Atlas: Few-shot learning\\nwith retrieval augmented language models. J. Mach.\\nLearn. Res. , 24:251:1–251:43.\\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\\nHwang, and Jong Park. 2023. Test-time self-adaptive\\nsmall language models for question answering. In'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 10}, page_content='Findings of the Association for Computational Lin-\\nguistics: EMNLP 2023, Singapore, December 6-10,\\n2023 , pages 15459–15469. Association for Computa-\\ntional Linguistics.\\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\\nCallan, and Graham Neubig. 2023. Active retrieval\\naugmented generation. In EMNLP 2023 .\\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 10}, page_content='supervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics, ACL\\n2017, Vancouver, Canada, July 30 - August 4, Volume\\n1: Long Papers , pages 1601–1611. Association for\\nComputational Linguistics.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\\nand Wen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 10}, page_content='the 2020 Conference on Empirical Methods in Natu-\\nral Language Processing, EMNLP 2020, November\\n16-20, 2020 . Association for Computational Linguis-\\ntics.\\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ro-\\nnan Le Bras, Akari Asai, Xinyan Yu, Dragomir R.\\nRadev, Noah A. Smith, Yejin Choi, and Kentaro Inui.2022. Realtime QA: what’s the answer right now?\\narXiv preprint arXiv:2207.13332 .\\nOmar Khattab, Keshav Santhanam, Xiang Lisa\\nLi, David Hall, Percy Liang, Christopher Potts,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 10}, page_content='and Matei Zaharia. 2022. Demonstrate-search-\\npredict: Composing retrieval and language mod-\\nels for knowledge-intensive NLP. arXiv preprint\\narXiv.2212.14024 , abs/2212.14024.\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\\nharwal. 2023. Decomposed prompting: A modular\\napproach for solving complex tasks. In The Eleventh\\nInternational Conference on Learning Representa-\\ntions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 .\\nOpenReview.net.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 10}, page_content='OpenReview.net.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, Kristina Toutanova, Llion Jones, Matthew\\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: A benchmark for question answering\\nresearch. Transactions of the Association for Compu-\\ntational Linguistics , 7:452–466.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 10}, page_content='Angeliki Lazaridou, Elena Gribovskaya, Wojciech\\nStokowiec, and Nikolai Grigorev. 2022. Internet-\\naugmented language models through few-shot\\nprompting for open-domain question answering.\\narXiv preprint arXiv:2203.05115 .\\nBelinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar\\nMehdad, and Wen-tau Yih. 2020. Efficient one-pass\\nend-to-end entity linking for questions. In Proceed-\\nings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing, EMNLP 2020, Online,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 10}, page_content='November 16-20, 2020 , pages 6433–6441. Associa-\\ntion for Computational Linguistics.\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\\nweight decay regularization. In 7th International\\nConference on Learning Representations, ICLR 2019,\\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\\nview.net.\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\\nWhen not to trust language models: Investigating\\neffectiveness of parametric and non-parametric mem-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 10}, page_content='ories. In Proceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), ACL 2023, Toronto, Canada,\\nJuly 9-14, 2023 , pages 9802–9822. Association for\\nComputational Linguistics.\\nOpenAI. 2023. GPT-4 technical report. arXiv preprint\\narXiv:2303.08774 .\\nAdam Paszke, Sam Gross, Francisco Massa, Adam\\nLerer, James Bradbury, Gregory Chanan, Trevor\\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\\nAntiga, Alban Desmaison, Andreas Köpf, Edward Z.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='Yang, Zachary DeVito, Martin Raison, Alykhan Te-\\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\\nimperative style, high-performance deep learning li-\\nbrary. In Advances in Neural Information Processing\\nSystems 32: Annual Conference on Neural Informa-\\ntion Processing Systems 2019 , pages 8024–8035.\\nJayr Alencar Pereira, Robson do Nascimento Fidalgo,\\nRoberto de Alencar Lotufo, and Rodrigo Frassetto'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='Nogueira. 2023. Visconde: Multi-document QA with\\nGPT-3 and neural reranking. In Advances in Informa-\\ntion Retrieval - 45th European Conference on Infor-\\nmation Retrieval, ECIR 2023, Dublin, Ireland, April\\n2-6, 2023, Proceedings, Part II , volume 13981 of\\nLecture Notes in Computer Science , pages 534–543.\\nSpringer.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A. Smith, and Mike Lewis. 2023. Measuring\\nand narrowing the compositionality gap in language'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='models. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2023 .\\nPeng Qi, Haejun Lee, Tg Sido, and Christopher D. Man-\\nning. 2021. Answering open-domain questions of\\nvarying reasoning steps from text. In Proceedings\\nof the 2021 Conference on Empirical Methods in\\nNatural Language Processing, EMNLP 2021, Vir-\\ntual Event / Punta Cana, Dominican Republic, 7-11\\nNovember, 2021 , pages 3599–3614. Association for\\nComputational Linguistics.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='Computational Linguistics.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. J. Mach. Learn. Res. , 21:140:1–140:67.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. Squad: 100, 000+ questions\\nfor machine comprehension of text. In Proceedings\\nof the 2016 Conference on Empirical Methods in'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='Natural Language Processing, EMNLP 2016, Austin,\\nTexas, USA, November 1-4, 2016 , pages 2383–2392.\\nThe Association for Computational Linguistics.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. Transactions of the Association for\\nComputational Linguistics .\\nStephen E. Robertson, Steve Walker, Susan Jones,\\nMicheline Hancock-Beaulieu, and Mike Gatford.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='1994. Okapi at TREC-3. In Proceedings of The Third\\nText REtrieval Conference, TREC 1994, Gaithers-\\nburg, Maryland, USA, November 2-4, 1994 , volume\\n500-225 of NIST Special Publication , pages 109–\\n126. National Institute of Standards and Technology\\n(NIST).\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\\nmoyer, and Wen-tau Yih. 2023. REPLUG: retrieval-augmented black-box language models. arXiv\\npreprint arXiv:2301.12652 .'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='preprint arXiv:2301.12652 .\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='Isabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='Melanie Kambadur, Sharan Narang, Aurélien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. 2023. Llama 2: Open foundation and fine-\\ntuned chat models. arXiv preprint arXiv:2307.09288 .\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2022a. Musique: Multi-\\nhop questions via single-hop question composition.\\nTrans. Assoc. Comput. Linguistics , 10:539–554.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2022b. ♪MuSiQue: Multi-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='hop questions via single-hop question composition.\\nTransactions of the Association for Computational\\nLinguistics , 10:539–554.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2023. Interleaving retrieval\\nwith chain-of-thought reasoning for knowledge-\\nintensive multi-step questions. In Proceedings of\\nthe 61st Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers),\\nACL 2023, Toronto, Canada, July 9-14, 2023 , pages'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='10014–10037. Association for Computational Lin-\\nguistics.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\\ngent abilities of large language models. Trans. Mach.\\nLearn. Res. , 2022.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 11}, page_content='and Denny Zhou. 2022b. Chain-of-thought prompt-\\ning elicits reasoning in large language models. In\\nNeurIPS .\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 12}, page_content='Joe Davison, Sam Shleifer, Patrick von Platen, Clara\\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\\nScao, Sylvain Gugger, Mariama Drame, Quentin\\nLhoest, and Alexander M. Rush. 2020. Transform-\\ners: State-of-the-art natural language processing. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing: System\\nDemonstrations, EMNLP 2020 - Demos , pages 38–\\n45. Association for Computational Linguistics.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 12}, page_content='Jialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2021. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval. In 9th International Conference on Learning\\nRepresentations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021 . OpenReview.net.\\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\\nEnd-to-end open-domain question answering with\\nbertserini. In Proceedings of the 2019 Conference'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 12}, page_content='of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\\nUSA, June 2-7, 2019, Demonstrations , pages 72–77.\\nAssociation for Computational Linguistics.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainable multi-hop question answering.\\nInProceedings of the 2018 Conference on Empiri-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 12}, page_content='cal Methods in Natural Language Processing , pages\\n2369–2380, Brussels, Belgium. Association for Com-\\nputational Linguistics.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik R. Narasimhan, and Yuan Cao. 2023.\\nReact: Synergizing reasoning and acting in language\\nmodels. In The Eleventh International Conference\\non Learning Representations, ICLR 2023, Kigali,\\nRwanda, May 1-5, 2023 . OpenReview.net.\\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 12}, page_content='Zheng, Soujanya Poria, and Tat-Seng Chua. 2021.\\nRetrieving and reading: A comprehensive survey on\\nopen-domain question answering. arXiv preprint\\narXiv:2101.00774 .'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 13}, page_content='0 2 4 6 8 10\\nTime per Query20304050Performance (F1)  No Retrieval  Single-step Approach\\n  Adaptive Retrieval  Multi-step Approach   Adaptive-RAG (Ours)Performance vs Time with FLAN-T5-XLFigure 4: QA performance (F1) and efficiency (Time/Query)\\nfor different retrieval-augmented generation approaches. We\\nuse the FLAN-T5-XL (3B) as the base LLM.\\nA Additional Experimental Setups\\nA.1 Datasets\\nWe use publicly open datasets for both single-\\nhop and multi-hop QA datasets, referring to'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 13}, page_content='as Karpukhin et al. (2020) and Trivedi et al. (2023),\\nrespectively. We describe the characteristics of\\neach dataset:\\n1) SQuAD v1.1 (Rajpurkar et al., 2016) is created\\nthrough a process where annotators write questions\\nbased on the documents they read.\\n2) Natural Questions (Kwiatkowski et al., 2019) is\\nconstructed by real user queries on Google Search.\\n3) TriviaQA (Joshi et al., 2017) comprises trivia\\nquestions sourced from various quiz websites.'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 13}, page_content='4) MuSiQue (Trivedi et al., 2022a) is collected by\\ncompositing multiple single-hop queries, to form\\nqueries spanning 2-4 hops.\\n5) HotpotQA (Yang et al., 2018) is constructed by\\nhaving annotators create questions that link multi-\\nple Wikipedia articles.\\n6) 2WikiMultiHopQA (Ho et al., 2020) is derived\\nfrom Wikipedia and its associated knowledge graph\\npath, needing 2-hops.\\nA.2 Models\\nWe describe the details of models as follows:\\n1) No Retrieval. This approach uses only the LLM'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 13}, page_content='itself, to generate the answer to the given query.\\n2) Single-step Approach. This approach first re-\\ntrieves the relevant knowledge with the given query\\nfrom the external knowledge sources and then aug-\\nments the LLM with this retrieved knowledge to\\ngenerate the answer, which iterates only once.\\n3) Adaptive Retrieval. This baseline (Mallen et al.,\\n2023) adaptively augments the LLM with the re-\\ntrieval module, only when the entities appearing\\nin queries are less popular. To extract entities, we'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 13}, page_content='use the available entity-linking method (Li et al.,\\n2020), namely BLINK, for questions.\\n4) Self-RAG. This baseline (Asai et al., 2024)\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\\nTime per Query304050Performance (F1)  No Retrieval  Single-step Approach\\n  Adaptive Retrieval  Multi-step Approach  Adaptive-RAG (Ours)Performance vs Time with FLAN-T5-XXLFigure 5: QA performance (F1) and efficiency (Time/Query)\\nfor different retrieval-augmented generation approaches. We'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 13}, page_content='use the FLAN-T5-XXL (11B) as the base LLM.\\ntrains the LLM to adaptively perform retrieval and\\ngeneration, where the retrieval is conducted once it\\npredicts the special retrieval token above a certain\\nthreshold, and the answer generation follows.\\n5) Adaptive-RAG. This is our model that adap-\\ntively selects the retrieval-augmented generation\\nstrategy, smoothly oscillating between the non-\\nretrieval, single-step approach, and multi-step ap-\\nproaches4without architectural changes, based on'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 13}, page_content='the query complexity assessed by the classifier.\\n6) Multi-step Approach. This approach (Trivedi\\net al., 2023) is the multi-step retrieval-augmented\\nLLM, which iteratively accesses both the retriever\\nand LLM with interleaved Chain-of-Thought rea-\\nsoning (Wei et al., 2022b) repeatedly until it derives\\nthe solution or reaches the maximum step number.\\n7) Adaptive-RAG w/ Oracle This is an ideal sce-\\nnario of our Adaptive-RAG equipped with an or-\\nacle classifier that perfectly categorizes the query'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 13}, page_content='complexity.\\nA.3 Implementation Details\\nFor computing resources, we use A100 GPUs\\nwith 80GB memory. In addition, due to the sig-\\nnificant costs associated with evaluating retrieval-\\naugmented generation models, we perform experi-\\nments with a single run. Finally, we implemented\\nmodels using PyTorch (Paszke et al., 2019) and\\nTransformers library (Wolf et al., 2020).\\nB Additional Experimental Results\\nPerformance vs Time We further provide a com-\\nparison of different retrieval-augmented genera-'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 13}, page_content='tion approaches with FLAN-T5-XL and FLAN-T5-\\nXXL models in Figure 4 and Figure 5, respectively,\\nin the context of performance and efficiency trade-\\noffs. Similar to the observation made from the GPT-\\n3.5 model in Figure 1, our proposed Adaptive-RAG\\nis significantly more effective as well as efficient.\\n4For the multi-step approach, we use the state-of-the-art\\nquestion answering strategy from IRCoT (Trivedi et al., 2023).'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 14}, page_content='Table 7: Results on each of a collection of datasets with FLAN-T5-XXL (11B) as the LLM. We emphasize our results in bold.\\nSQuAD Natural Questions TriviaQA\\nData Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nSingle-stepSimpleNo Retrieval 7.00 14.40 8.40 0.00 0.08 18.80 25.50 20.40 0.00 0.08 32.80 39.20 35.40 0.00 0.08\\nSingle-step Approach 28.80 40.80 35.00 1.00 1.00 41.40 51.20 47.60 1.00 1.00 56.00 64.70 61.80 1.00 1.00'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 14}, page_content='AdaptiveAdaptive Retrieval 15.60 25.60 20.00 0.50 0.54 31.00 39.70 35.00 0.50 0.54 44.80 52.20 48.60 0.50 0.54\\nSelf-RAG∗1.60 11.90 20.80 0.59 0.31 39.20 47.10 42.40 0.75 0.09 14.60 33.70 60.20 0.76 0.22\\nAdaptive-RAG (Ours) 27.80 39.80 34.00 1.17 1.50 41.20 51.00 47.40 1.00 1.00 52.00 60.30 57.20 1.03 1.33\\nComplex Multi-step Approach 24.60 36.90 30.20 2.13 3.83 39.60 49.60 46.40 2.16 3.94 52.60 61.10 59.40 2.17 4.03'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 14}, page_content='Oracle Adaptive-RAG w/ Oracle 32.80 46.90 38.20 0.85 0.94 51.20 61.00 57.00 0.71 0.91 63.40 71.30 68.20 0.51 0.60\\nMuSiQue HotpotQA 2WikiMultiHopQA\\nData Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nMulti-stepSimpleNo Retrieval 4.20 13.40 5.40 0.00 0.08 17.40 25.44 18.40 0.00 0.09 26.80 32.93 28.00 0.00 0.08\\nSingle-step Approach 16.80 25.70 19.20 1.00 1.00 37.60 49.27 39.60 1.00 1.00 46.60 54.13 48.20 1.00 1.00'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 14}, page_content='AdaptiveAdaptive Retrieval 8.40 17.80 10.20 0.50 0.54 26.60 36.01 27.80 0.50 0.54 35.20 42.68 36.80 0.50 0.54\\nSelf-RAG∗1.20 8.20 11.80 0.68 0.27 5.60 17.86 30.60 0.76 0.26 3.00 19.14 39.00 0.90 0.25\\nAdaptive-RAG (Ours) 20.60 28.50 23.20 1.89 3.12 44.20 54.78 46.80 1.58 2.53 47.60 57.36 54.00 1.46 2.55\\nComplex Multi-step Approach 19.40 27.50 21.80 2.09 3.66 47.00 57.81 49.40 2.08 3.73 57.60 67.65 64.00 2.17 3.63'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 14}, page_content='Oracle Adaptive-RAG w/ Oracle 24.20 37.20 26.60 1.22 1.71 52.20 64.80 54.60 0.92 1.33 59.20 70.40 68.60 0.82 1.14\\nTable 8: Results on each of a collection of datasets with GPT-3.5 (Turbo) as the LLM. We emphasize our results in bold.\\nSQuAD Natural Questions TriviaQA\\nData Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nSingle-stepSimpleNo Retrieval 16.00 29.20 23.80 0.00 0.62 39.80 55.70 55.00 0.00 0.56 64.00 75.60 75.80 0.00 0.68'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 14}, page_content='Single-step Approach 18.00 33.80 29.20 1.00 1.00 32.40 46.80 54.80 1.00 1.00 55.20 66.50 65.80 1.00 1.00\\nAdaptiveAdaptive Retrieval 15.40 30.00 24.40 0.50 0.81 36.40 51.20 56.60 0.50 0.78 62.00 71.90 72.20 0.50 0.84\\nSelf-RAG∗1.60 11.90 20.80 0.59 1.91 39.20 47.10 42.40 0.75 0.52 14.60 33.70 60.20 0.76 1.59\\nAdaptive-RAG (Ours) 19.80 34.40 30.00 0.87 1.21 36.80 52.00 56.60 0.68 0.86 62.40 73.80 73.80 0.22 0.79'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 14}, page_content='Complex Multi-step Approach 17.40 31.50 26.20 2.50 3.24 35.60 49.70 57.80 2.58 3.79 54.80 67.10 68.00 2.30 2.65\\nOracle Adaptive-RAG w/ Oracle 28.00 45.90 39.40 0.54 0.93 50.00 65.40 67.00 0.28 0.8 70.80 81.00 80.00 0.11 0.73\\nMuSiQue HotpotQA 2WikiMultiHopQA\\nData Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time\\nMulti-stepSimpleNo Retrieval 20.40 31.30 24.40 0.00 0.81 37.40 51.04 43.20 0.00 0.74 37.00 48.50 43.40 0.00 0.90'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 14}, page_content='Single-step Approach 16.40 26.70 23.60 1.00 1.00 39.60 50.44 45.60 1.00 1.00 46.80 57.69 52.60 1.00 1.00\\nAdaptiveAdaptive Retrieval 18.80 30.30 24.80 0.50 0.90 38.60 50.70 43.20 0.50 0.87 44.20 55.11 50.60 0.50 0.95\\nSelf-RAG∗1.20 8.20 11.80 0.68 1.66 5.60 17.86 30.60 0.76 1.67 3.00 19.14 39.00 0.90 1.81\\nAdaptive-RAG (Ours) 21.80 32.60 29.60 1.90 2.29 40.40 52.56 47.00 0.93 1.48 46.60 60.09 56.80 1.59 2.23'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 14}, page_content='Complex Multi-step Approach 23.00 32.50 31.60 3.41 3.61 45.80 58.36 52.20 2.73 3.18 52.20 66.08 62.40 3.36 3.35\\nOracle Adaptive-RAG w/ Oracle 29.60 44.70 35.60 0.90 1.45 55.60 69.90 62.80 0.54 1.08 52.20 69.90 66.60 0.65 1.21\\nPerformance per Dataset In addition to detail-\\ning the performance of each dataset with the FLAN-\\nT5-XL model, as shown in Table 2, we also present\\nthe results for each dataset with the FLAN-T5-\\nXXL and GPT-3.5 models in Table 2 and Table 8,'),\n",
       " Document(metadata={'source': 'RAG.pdf', 'page': 14}, page_content='respectively. The experimental results show that\\nour Adaptive-RAG consistently balances between\\nefficiency and accuracy. It is worth noting that\\nwhile the GPT-3.5 model performs effectively in\\naddressing straightforward queries even without\\ndocument retrieval, it benefits significantly from\\nour Adaptive-RAG in terms of effectiveness when\\nsolving complex multi-hop queries.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "text_chunks = text_splitter.split_documents(data)\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'RAG.pdf', 'page': 0}, page_content='LLMs, have emerged as a promising approach\\nto enhancing response accuracy in several tasks,\\nsuch as Question-Answering (QA). However,\\neven though there are various approaches deal-\\ning with queries of different complexities, they\\neither handle simple queries with unnecessary\\ncomputational overhead or fail to adequately\\naddress complex multi-step queries; yet, not\\nall user requests fall into only one of the sim-\\nple or complex categories. In this work, we')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "vectorstore_from_docs = PineconeVectorStore.from_documents(\n",
    "        text_chunks,\n",
    "        index_name=index_name,\n",
    "        embedding=embedding_model\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 6.0, 'source': 'RAG.pdf'}, page_content='sample and annotate 400 queries from 6 datasets\\nbased on its inductive bias (single-hop for one-step\\napproach and multi-hop for multi-step). In addition,\\nwe use predicted outcomes of three different strate-\\ngies over 400 queries sampled from each dataset.\\nNote that those queries used for classifier training\\ndo not overlap with the testing queries for QA.\\n5 Experimental Results and Analyses\\nIn this section, we show the overall experimental\\nresults and offer in-depth analyses of our method.'),\n",
       " Document(metadata={'page': 1.0, 'source': 'RAG.pdf'}, page_content='a query, this single-step approach retrieves relevant documents and then generates an answer. However, it may not be sufficient\\nfor complex queries that require multi-step reasoning. (B) This multi-step approach iteratively retrieves documents and generates\\nintermediate answers, which is powerful yet largely inefficient for the simple query since it requires multiple accesses to both')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is single-step approach\"\n",
    "vectorstore_from_docs.similarity_search(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm , chain_type=\"stuff\",retriever = vectorstore_from_docs.as_retriever())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is single-step approach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The single-step approach is a method for answering queries that involves retrieving relevant documents and then generating an answer in a single step. This approach is suitable for simple queries that don't require complex reasoning. \\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
