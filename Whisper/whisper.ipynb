{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper : speech-to-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (from groq) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\asus\\anaconda3\\envs\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Groq client\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "client = Groq(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"audio_test.m4a\"  #specify the path to the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the audio file\n",
    "\n",
    "with open(filename,'rb') as file:\n",
    "    # Create a transcription of the audio file\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        file = (filename, file.read()),\n",
    "        model = \"whisper-large-v3\",\n",
    "        #model = \"distil-whisper-large-v3-en\",\n",
    "        prompt = \"Specify context or spelling\",\n",
    "        response_format= \"json\",\n",
    "        language= \"de\",\n",
    "        temperature= 0.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hallo, alle! In diesem Video werden wir heute über Grog reden. Lass uns beginnen. Wie ihr gesehen habt, haben wir in den letzten Videos viele LLM Modelle benutzt. Wir haben auch mit dem Paid Modell gearbeitet und mit dem Open Source Modell. Wenn ich über Paid sage, haben wir das Gemini Modell benutzt, das von Google kommt. Wir haben das Cloudy Modell benutzt, das von Anthropic kommt. Wir haben auch mit einigen Open Source Modellen gearbeitet. Wir haben in Open Source O-Lama geöffnet. In O-Lama haben wir viele Open Source Modelle geöffnet. Ob es Gamma, Met, Lama oder Mistral ist. So haben wir auch Open Source Modelle geöffnet. Wir haben auch mit Hugging Face LLM Modelle geöffnet. Wir haben jetzt einen anderen Weg. Wenn wir einen Modell nutzen wollen, besonders Open Source Modelle, können wir ihn mit der Hilfe von Grog nutzen. Grok bietet euch eine API, die euch hilft, welche Modelle ihr benutzen wollt. Egal ob Lama oder Gamma. Egal ob Lama 3 oder Lama 3.2. Ihr könnt diese Modelle nutzen mit der Hilfe von Grok. Grok hat eine Firma gemacht, eine AI Firma. und das ist 2016 das ist die Firma, die 2016 geschaffen wurde das war ein Google Former Engineer Jonathan Ross er hat die Firma geschaffen die Leute fokussierten sich auf TPUs jetzt ist es so, dass die LLM sehr gegrössert wird jede Firma will eine LLM App machen und wie alle unsere Firmen OpenAI, Anthropic, Meta Jede Firma will ein bestes LLM Modell herstellen Was ist Grog hier? Er hostet diese LLM Modelle in der Cloud Wie Sie gesehen haben, downloaden Sie das Olamo Modell Wenn Sie einen Open Source Modell nutzen wollen Dann downloaden Sie das Olamo Modell Und dann nutzen Sie das Modell Und dann pullen Sie es Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. Das ist ein Vorteil des Local Machine Downloads. und das Hauptwahrnehmende ist, wie man es nutzen kann. Wie man es nutzen kann, ich zeige euch den Link auf grogg.com, ihr findet ihn einfach. Ich zeige euch, wie wir es nutzen können. Denn am Ende m wir nur die LLM Modelle nutzen und wir m den EI Case l Was man tun muss ist man muss nur auf diesen Entwicklungslink klicken und we just need to use the LLM models and we need to solve the EI use case So what you need to do you just need to click on this developer link and just click on the start building So after clicking on the start building, so in my case actually it not ask any credential because I have already logged in with my account. So what you just need to do, Wenn du hier klickst, wird es dir sagen, dass du dich in den incognito mode bewegen kannst. Ich habe bereits eingeloggt, deshalb hat er nicht gesagt. Ich zeige es dir. Er sagt, dass du eingeloggt bist. Du musst einfach nur mit deinem Google-Account eingeloggt werden. Das ist ein sehr einfacher Schritt. Sobald du eingeloggt bist, wird das erste Fenster, das du hast, so öffnen. Dann was du tun musst, du kannst sehen, dass das erste ist Playground. Wie wir es in den restlichen LLM-Modellen gesehen haben. In OpenAI, Anthropic oder Google. in der Playground-Applikation. Aber unsere Hauptfunktion ist, wir müssen die API-Key benutzen. Und ihr könnt eine simple API-Key machen. Jetzt könnt ihr sehen, ich habe gerade eine API-Key gemacht. Und ihr könnt auch eine simple API-Key machen. Ihr könnt ihr Name geben. Und dann könnt ihr sie einfach in eurem Notepad erwerben. Macht sicher, dass wenn ihr eine API-Key macht, alle LLM-Modelle, die wir bis jetzt gearbeitet haben, ihr die API-Key in eurem Notepad erwerben. So, wie werden wir das jetzt nutzen? Wir werden es nun mit Langchain integrieren. Es sind sehr einfache Dinge, die wir bis jetzt gelernt haben. Wir werden nur eine Bibliothek benutzen. Die restlichen Dinge sind gleich. Ich zeige euch noch mehr Dinge, damit ihr eine Idee bekommt, welche Modelle wir in Grok nutzen können. Wenn ihr hier klickt, findet ihr viele Modelle. Google hat ein Modell, das auf 7 Billion Parameter trainiert ist. Gamma 2 ist trainiert auf 9 Billion Parameter. Dann gibt es das Modell von Lama 3. Es gibt ein Modell mit dem Lama 3. Dann gibt es das Modell von Hugging Face Whisper. Lama 3.1. All diese Modelle, die Lama nennt, können Sie auch nutzen. Lama Guard, das ist ein Modell von Gassam. Das könnt ihr auch nutzen. Und Mistral ist ein Modell. Das ist ein sehr gutes Modell. Es gibt ein Modell von OpenAI, das Whisper ist. Ich werde es euch in den nächsten Videos zeigen. Wir werden es in der Sprachübertragung machen. Und er wird alles was ihr sagt das wird er in den Text verwenden Wir werden das weiter anschauen Im n Video wo wir das N2N Projekt machen werde ich Grog benutzen. Denn in den letzten Videos habe ich O-Lama Models und Gemini Models benutzt. Wenn ihr die APA aufgestellt habt, und was du tun musst, du musst einfach in deinem VS Code kommen und hier habe ich dir ein .env File gemacht in dem ich ein grok API habe, das ich dir erklärt habe, wie wir das set upen müssen und ich habe ein requirement.txt File gemacht in dem ich langchain, langchain grok und python.env habe und hier habe ich einfach pip install hyphen r gemacht und alle meine Requirements installiert habe Ich habe es in den letzten Videos gelernt. Ich wollte nur sagen, wie man das Open Source Modell nutzen kann mit dem Hilfe von Grok. Denn die anderen Dinge sind gleich. Ich habe hier eine Bibliothek importiert. Ich habe es von der Langchain Grok importiert. Weil ich Chatmodelle nutzen muss. Dann habe ich eine LLM-Chain gemacht. Ich habe ein Prompt-Template gemacht. und die OS und die .env und die .loadenv die ich für die APK loaden muss hier habe ich meine APK geloadet von OS.inv und OS.getenv und habe meine APK gelegt jetzt seht ihr, was ich gemacht habe ich habe einen Chatmodel gelegt und habe ihn in die Sully-Klasse gelegt und der erste Parameter, der in den Chatmodel geht, ist Modellname und Temperatur Temperatur kann man auch einstellen Temperatur ist generell die Kreativität oder die Unabhängigkeit Ich habe hier 0.7 gegeben, also ist der Modell kreativer. Jetzt könnt ihr hier benutzen, wie ich hier jetzt habe, Gamma zu 9 Billion Parameter. Ihr könnt also auch Mistel oder Lama 3 benutzen. Ihr könnt auch jedes Modell benutzen, es ist euch egal. Dann seht ihr, was hier passiert, dass ich ein Promptemplate erhalte. Und in diesem Promptemplate habe ich zuerst eine Input-Variante eingegeben, dass ich eine Frage fragen werde. Ich habe ein Template-Cap eingegeben. Ich habe ein Template eingebaut, das ich bitte beantworten darf. Das heißt, okay, das sind alles Dinge, die du kennst, weil das ist nichts Neues für dich. Okay, nachdem ich das gemacht habe, habe ich einfach meine LLM-Klasse angerufen. Und was werde ich in die Klasse geben? In die Klasse wird zuerst mein LLM-Modell eingebaut und dann wird mein Prompt eingebaut, den ich hier im Grunde genommen habe. Du weißt, warum wir ein Prompt-Template machen. Ich frage jetzt eine Frage, und ich habe eine neue Frage in die Input-Variable eingebaut. und in die Terminierung schreibt, bitte beantworten diese Fragen. Okay. Jetzt muss ich sie beantworten. Ich habe hier eine Frage als String eingeführt. Was sind die drei Hauptbedürfnisse der Artifizierten Intelligenz? Ich werde diese Chain beantworten und diese Frage als Diktator beantworten. Ihr seht ich habe sie beantwortet Ich habe die Frage hier geprintet Okay dann machen wir das mal dem Bild Ich habe die Frage auf dem Bild Ich habe die Frage auf dem Bild Ich habe die Frage auf dem Bild Ich habe die Frage auf dem Bild. Ich habe die Frage auf dem Bild. Ich habe die Frage auf dem Bild. Ich habe die Frage auf dem Bild. Ich habe die Frage auf dem Bild. Ich habe die Frage auf dem Bild. Ich habe die Frage auf dem Bild. Ich habe die Frage auf dem Bild. Ich habe die Frage auf dem Bild. und wie er das antwortet. Lass uns sagen, ich benutze den Modell Mistral. Ich hätte von Anfang an mit Scratch schreiben können, aber wir haben es in den letzten Videos gesehen. Jetzt müsst ihr es nur verstehen, wie ihr es seht, was Prompt, Upright ist, was Chain ist. correct main was was, dass man chat, gro, geo, class, use, wo der erste parameter ist, der Model Name right So, ich lasse es jetzt run. Jetzt schauen wir uns die Antwort an. Wir benutzen jetzt ein Modell. Es hat Effizientität und Produktivität, Datenanalysen, Insights und Entwicklung in der Gesundheit und Wissenschaft gegeben. So, es hat diese Antwort gegeben. Nun können Sie das Modell von Mr. Alkohol nutzen. Wir nutzen auch das Modell von Lama. Ihr könnt jedes Modell nutzen, mit der Hilfe von uns. Es muss nur ein APA gebildet werden. und dann musst du verschiedene Modelle downloaden und dann musst du die Modelle pullen und jetzt kommt die Antwort welches Modell benutzt du? Lama Lama ist ein sehr guter Modell und es ist eine sehr gute Konkurrenz zwischen OpenAI und Lama weil der Workbench Score von Lama 3 war im Grunde gleich und sogar auch gut auf OpenEA Lama Models sind auch sehr gute Modelle und das Beste an Lama Models ist, dass sie Open Source sind Ihr seht hier, dass ihr keine API-Karte braucht, die ihr nicht bezahlen müsst Ihr braucht keine Karte bezahlen Ihr braucht keine Kreditkarte, wo ihr euch nicht interessiert, ob ihr mehr Kreditkarten benutzen könnt Hier benutzt man es in einer sehr einfachen Weise. Das Problem ist, dass man die Dinge Schritt für Schritt verstehen muss. Sie haben gesehen, wie einfach wir das gemacht haben. Das war die zweite Frage. Ich habe sie auch gefragt, wie das Machine Learning funktioniert. Er hat mir gesagt, dass Machine Learning ein Subfield ist, das die Computer lernen kann, ohne dass sie aus dem Programm ausgewertet werden. Und dann gibt es die Basis-Komponenten, wie die Daten, Modellbildung, Training, Modellbildung, Testen, So I hope you understood In the next video we will use the Grok API So you set it up If you have any doubts, please ask me in the comment section So see you all in my next video Thank you\n"
     ]
    }
   ],
   "source": [
    "# Print the transcription text\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fpdf\n",
      "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fpdf\n",
      "  Building wheel for fpdf (setup.py): started\n",
      "  Building wheel for fpdf (setup.py): finished with status 'done'\n",
      "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40714 sha256=8c38000b3c061026ae7cecd4c75c3684dfd0d08943d213a9b145d558246ad631\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\6e\\62\\11\\dc73d78e40a218ad52e7451f30166e94491be013a7850b5d75\n",
      "Successfully built fpdf\n",
      "Installing collected packages: fpdf\n",
      "Successfully installed fpdf-1.7.2\n"
     ]
    }
   ],
   "source": [
    "! pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to transcription.pdf\n"
     ]
    }
   ],
   "source": [
    "# Function to save transcription to a PDF file\n",
    "\n",
    "def save_to_pdf(text,output_pdf = 'transcription.pdf'):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\",size=12)\n",
    "    \n",
    "    # Add each line to the PDF\n",
    "    for line in text.splitlines():\n",
    "        pdf.multi_cell(0,10,line)  # Add line with authomatic wrapping\n",
    "        \n",
    "    pdf.output(output_pdf)\n",
    "    print(f\"Transcription saved to {output_pdf}\")\n",
    "    \n",
    "with open(filename,'rb') as file:\n",
    "    # Create a transcription of the audio file\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        file = (filename, file.read()),\n",
    "        #model = \"whisper-large-v3\",\n",
    "        model = \"distil-whisper-large-v3-en\",\n",
    "        prompt = \"Specify context or spelling\",\n",
    "        response_format= \"json\",\n",
    "        language= \"en\",\n",
    "        temperature= 0.0\n",
    "    )\n",
    "# Optionally, split the transcription into sentences (if needed)\n",
    "    transcription_text = transcription.text\n",
    "    \n",
    "# Save the transcription text to a PDF file\n",
    "    \n",
    "    save_to_pdf(transcription_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to transcription.pdf\n"
     ]
    }
   ],
   "source": [
    "# Function to split text by punctuation for script-wise splitting\n",
    "def split_script_wise(text):\n",
    "    import re\n",
    "    # Split based on punctuation (period, exclamation, question mark, etc.)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return sentences\n",
    "\n",
    "# Function to save transcription to a PDF file with formatting\n",
    "def save_to_pdf(text, title=\"Meeting Transcription\", timeframe=\"Duration: 10:00 AM - 11:00 AM\", output_pdf=\"transcription.pdf\"):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "\n",
    "    # Add Title\n",
    "    pdf.set_font(\"Arial\", 'B', 16)\n",
    "    pdf.cell(0, 10, title, ln=True, align='C')\n",
    "    \n",
    "    # Add Timeframe\n",
    "    pdf.set_font(\"Arial\", 'I', 12)\n",
    "    pdf.cell(0, 10, timeframe, ln=True, align='C')\n",
    "\n",
    "    pdf.ln(10)  # Add space after title and timeframe\n",
    "\n",
    "    # Set font for the body\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "    # Split the text into script-wise sentences\n",
    "    sentences = split_script_wise(text)\n",
    "\n",
    "    # Add each sentence as a new line to the PDF\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        # Adding sentence index to simulate script-wise formatting (optional)\n",
    "        pdf.multi_cell(0, 10, f\"{idx + 1}. {sentence.strip()}\")\n",
    "    \n",
    "    pdf.output(output_pdf)\n",
    "    print(f\"Transcription saved to {output_pdf}\")\n",
    "\n",
    "# Open the audio file\n",
    "with open(filename, \"rb\") as file:\n",
    "    # Create a transcription of the audio file\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        file=(filename, file.read()),  # Required audio file\n",
    "        model=\"distil-whisper-large-v3-en\",  # Required model to use for transcription\n",
    "        prompt=\"Specify context or spelling\",  # Optional\n",
    "        response_format=\"json\",  # Optional\n",
    "        language=\"en\",  # Optional\n",
    "        temperature=0.0  # Optional\n",
    "    )\n",
    "\n",
    "    # Get the transcription text\n",
    "    transcription_text = transcription.text\n",
    "    \n",
    "    # Save the transcription text to a PDF file with formatting\n",
    "    save_to_pdf(transcription_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to transcription.pdf\n"
     ]
    }
   ],
   "source": [
    "# Function to split text by punctuation for script-wise splitting\n",
    "def split_script_wise(text):\n",
    "    import re\n",
    "    # Split based on punctuation (period, exclamation, question mark, etc.)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return sentences\n",
    "\n",
    "# Function to save transcription to a PDF file with colors and formatting\n",
    "def save_to_pdf(text, title=\"Meeting Transcription\", timeframe=\"Duration: 10:00 AM - 11:00 AM\", output_pdf=\"transcription.pdf\"):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "\n",
    "    # Add Title with Background Color\n",
    "    pdf.set_font(\"Arial\", 'B', 16)\n",
    "    pdf.set_text_color(255, 255, 255)  # White text\n",
    "    pdf.set_fill_color(0, 102, 204)  # Dark blue background\n",
    "    pdf.cell(0, 10, title, ln=True, align='C', fill=True)\n",
    "    \n",
    "    # Add Timeframe with Light Gray Background\n",
    "    pdf.ln(5)  # Space between title and timeframe\n",
    "    pdf.set_font(\"Arial\", 'I', 12)\n",
    "    pdf.set_text_color(0, 0, 0)  # Black text\n",
    "    pdf.set_fill_color(230, 230, 230)  # Light gray background\n",
    "    pdf.cell(0, 10, timeframe, ln=True, align='C', fill=True)\n",
    "\n",
    "    pdf.ln(10)  # Add space after title and timeframe\n",
    "\n",
    "    # Set font for the body and colors\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.set_text_color(0, 0, 0)  # Black text for body\n",
    "\n",
    "    # Split the text into script-wise sentences\n",
    "    sentences = split_script_wise(text)\n",
    "\n",
    "    # Alternate background color for script sections\n",
    "    fill_color = False  # To toggle background color for alternate sections\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        # Toggle background color for every alternate sentence\n",
    "        if idx % 2 == 0:\n",
    "            pdf.set_fill_color(245, 245, 245)  # Light background\n",
    "            fill_color = True\n",
    "        else:\n",
    "            fill_color = False\n",
    "\n",
    "        # Add each sentence as a new line with alternating background\n",
    "        pdf.multi_cell(0, 10, f\"{idx + 1}. {sentence.strip()}\", fill=fill_color)\n",
    "    \n",
    "    pdf.output(output_pdf)\n",
    "    print(f\"Transcription saved to {output_pdf}\")\n",
    "\n",
    "# Open the audio file\n",
    "with open(filename, \"rb\") as file:\n",
    "    # Create a transcription of the audio file\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        file=(filename, file.read()),  # Required audio file\n",
    "        model=\"whisper-large-v3\",  # Required model to use for transcription\n",
    "        prompt=\"Specify context or spelling\",  # Optional\n",
    "        response_format=\"json\",  # Optional\n",
    "        language=\"en\",  # Optional\n",
    "        temperature=0.0  # Optional\n",
    "    )\n",
    "\n",
    "    # Get the transcription text\n",
    "    transcription_text = transcription.text\n",
    "    \n",
    "    # Save the transcription text to a PDF file with formatting and colors\n",
    "    save_to_pdf(transcription_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
